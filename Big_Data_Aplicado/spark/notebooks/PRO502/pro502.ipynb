{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee55ee1",
   "metadata": {},
   "source": [
    "## Dataset 1: Datos para la predicción del rendimiento en cultivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6828ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/PRO502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd PRO502/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d1c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/23 09:40:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/local/lib/python3.10/socket.py\", line 717, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "26/01/23 09:40:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StructType, StructField\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringType, IntegerType, FloatType,BooleanType\n\u001b[0;32m----> 6\u001b[0m spark \u001b[38;5;241m=\u001b[39m ( \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPRO502\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark://spark-master:7077\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m sc \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1586\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1578\u001b[0m args_command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m   1579\u001b[0m     [get_command_part(arg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m new_args])\n\u001b[1;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1588\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn)\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 09:40:38 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType,BooleanType\n",
    "\n",
    "\n",
    "spark = ( SparkSession.builder.appName(\"PRO502\").master(\"spark://spark-master:7077\").getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e158407",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_schema = StructType([\n",
    "    StructField(\"Crop\",StringType(),True),\n",
    "    StructField(\"Region\",StringType(),True),\n",
    "    StructField(\"Soil_Type\",StringType(),True),\n",
    "    StructField(\"Soil_pH\",FloatType(),True),\n",
    "    StructField(\"Rainfall_mm\",FloatType(),True),\n",
    "    StructField(\"Temperature_C\",FloatType(),True),\n",
    "    StructField(\"Humidity_pct\",FloatType(),True),\n",
    "    StructField(\"Fertilizer_Used_Kg\",FloatType(),True),\n",
    "    StructField(\"Irrigation\",StringType(),True),\n",
    "    StructField(\"Pesticides_Used_kg\",FloatType(),True),\n",
    "    StructField(\"Planting_Density\",FloatType(),True),\n",
    "    StructField(\"Previus_Crop\",StringType(),True),\n",
    "    StructField(\"Yield_ton_per_ha\",FloatType(),True)\n",
    "])\n",
    "\n",
    "df_crop = (spark.read.format(\"csv\")\n",
    "      .option(\"header\",\"true\")\n",
    "      .schema(main_schema)\n",
    "      .load(\"crop_yield_dataset.csv\"))\n",
    "\n",
    "\n",
    "df_sel = df_crop.select(\"Crop\",\"Region\",\"Temperature_C\",\"Rainfall_mm\",\"Irrigation\",\"Yield_ton_per_ha\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77faa287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+------+----------+-----------+\n",
      "|  Crop|  Region|Temperatura|Lluvia|Irrigation|Rendimiento|\n",
      "+------+--------+-----------+------+----------+-----------+\n",
      "| Maize|Region_C|       19.7|1485.4|      Drip|     101.48|\n",
      "|Barley|Region_D|       29.1| 399.4| Sprinkler|     127.39|\n",
      "|  Rice|Region_C|       30.5| 980.9| Sprinkler|      68.99|\n",
      "| Maize|Region_D|       26.4|1054.3|      Drip|     169.06|\n",
      "| Maize|Region_D|       20.4| 744.6|      Drip|     118.71|\n",
      "+------+--------+-----------+------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_renamed = (df_sel\n",
    "    .withColumnRenamed(\"Temperature_C\", \"Temperatura\")\n",
    "    .withColumnRenamed(\"Rainfall_mm\", \"Lluvia\")\n",
    "    .withColumnRenamed(\"Yield_ton_per_ha\", \"Rendimiento\")\n",
    ")\n",
    "df_renamed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f991649a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----------+------+----------+-----------+\n",
      "| Crop|  Region|Temperatura|Lluvia|Irrigation|Rendimiento|\n",
      "+-----+--------+-----------+------+----------+-----------+\n",
      "|Maize|Region_C|       19.7|1485.4|      Drip|     101.48|\n",
      "|Maize|Region_D|       26.4|1054.3|      Drip|     169.06|\n",
      "|Maize|Region_D|       20.4| 744.6|      Drip|     118.71|\n",
      "|Maize|Region_C|       32.4| 846.1|      None|      162.2|\n",
      "|Maize|Region_C|       18.3| 209.5|     Flood|     116.72|\n",
      "+-----+--------+-----------+------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_filtered = df_renamed.filter(\n",
    "    (col(\"Crop\") == \"Maize\") & \n",
    "    (col(\"Temperatura\") > 5)\n",
    ")\n",
    "\n",
    "df_filtered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e1dd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----------+------+----------+-----------+\n",
      "| Crop|  Region|Temperatura|Lluvia|Irrigation|Rendimiento|\n",
      "+-----+--------+-----------+------+----------+-----------+\n",
      "|Maize|Region_D|       26.4|1054.3|      Drip|     169.06|\n",
      "|Maize|Region_C|       32.4| 846.1|      None|      162.2|\n",
      "|Maize|Region_A|       26.6| 362.5| Sprinkler|      95.23|\n",
      "|Maize|Region_C|       33.7|1193.3|      None|     110.57|\n",
      "|Maize|Region_C|       27.8| 695.2|     Flood|     143.84|\n",
      "+-----+--------+-----------+------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final = (df_sel\n",
    "    .withColumnRenamed(\"Temperature_C\", \"Temperatura\")\n",
    "    .withColumnRenamed(\"Rainfall_mm\", \"Lluvia\")\n",
    "    .withColumnRenamed(\"Yield_ton_per_ha\", \"Rendimiento\")\n",
    "    .filter((col(\"Crop\") == \"Maize\") & (col(\"Temperatura\") > 25))\n",
    ")\n",
    "\n",
    "# Visualizamos el resultado final\n",
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22a061d",
   "metadata": {},
   "source": [
    "## Dataset 2: Lugares famosos del mundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf505b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_schema = StructType([\n",
    "    StructField(\"Place_Name\",StringType(),True),\n",
    "    StructField(\"Country\",StringType(),True),\n",
    "    StructField(\"City\",StringType(),True),\n",
    "    StructField(\"Annual_Visitors_Millions\",FloatType(),True),\n",
    "    StructField(\"Type\",StringType(),True),\n",
    "    StructField(\"Unsesco_World_Heritage\",StringType(),True),\n",
    "    StructField(\"Year_Built\",StringType(),True),\n",
    "    StructField(\"Entry_Fee_USD\",StringType(),True),\n",
    "    StructField(\"Best_Visit_Month\",StringType(),True),\n",
    "    StructField(\"Region\",StringType(),True),\n",
    "    StructField(\"Tourism_Revenue_Million_USD\",IntegerType(),True),\n",
    "    StructField(\"Average_Visit_Duration_Hours\",FloatType(),True),\n",
    "    StructField(\"Famous_For\",StringType(),True),\n",
    "])\n",
    "\n",
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"header\",\"true\")\n",
    "      .schema(main_schema)\n",
    "      .load(\"world_famous_places_2024.csv\"))\n",
    "\n",
    "df_base = df.select(\"Place_Name\",\"Country\",\"Unsesco_World_Heritage\",\"Entry_Fee_USD\",\"Annual_Visitors_Millions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba8af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+---------+--------------+-------------------+\n",
      "|               Lugar|      Country|Es_UNESCO|Precio_Entrada|Visitantes_Millones|\n",
      "+--------------------+-------------+---------+--------------+-------------------+\n",
      "|        Eiffel Tower|       France|       No|            35|                7.0|\n",
      "|        Times Square|United States|       No|             0|               50.0|\n",
      "|       Louvre Museum|       France|      Yes|            22|                8.7|\n",
      "| Great Wall of China|        China|      Yes|            10|               10.0|\n",
      "|           Taj Mahal|        India|      Yes|            15|                7.5|\n",
      "|           Colosseum|        Italy|      Yes|            18|               7.65|\n",
      "|   Statue of Liberty|United States|      Yes|            25|                4.3|\n",
      "|  Sydney Opera House|    Australia|      Yes|            49|                8.2|\n",
      "|        Machu Picchu|         Peru|      Yes|            70|                1.5|\n",
      "|      Forbidden City|        China|      Yes|             8|                9.0|\n",
      "|          Angkor Wat|     Cambodia|      Yes|            37|                2.6|\n",
      "|Notre-Dame Cathedral|       France|      Yes|             0|               13.0|\n",
      "|        Central Park|United States|       No|             0|               42.0|\n",
      "|     Las Vegas Strip|United States|       No|             0|               41.7|\n",
      "|Empire State Buil...|United States|       No|            44|                4.6|\n",
      "|  Golden Gate Bridge|United States|       No|             0|               15.0|\n",
      "|    Lincoln Memorial|United States|       No|             0|                8.5|\n",
      "|     Sagrada Familia|        Spain|      Yes|            26|                4.7|\n",
      "|Disneyland (Calif...|United States|       No|           104|               16.0|\n",
      "|Magic Kingdom (Or...|United States|       No|           109|               17.0|\n",
      "+--------------------+-------------+---------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_es = (df_base\n",
    "    .withColumnRenamed(\"Place_Name\", \"Lugar\")\n",
    "    .withColumnRenamed(\"Unsesco_World_Heritage\", \"Es_UNESCO\")\n",
    "    .withColumnRenamed(\"Entry_Fee_USD\", \"Precio_Entrada\")\n",
    "    .withColumnRenamed(\"Annual_Visitors_Millions\", \"Visitantes_Millones\")\n",
    ")\n",
    "df_es.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490cb333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---------+--------------+-------------------+\n",
      "|               Lugar|Country|Es_UNESCO|Precio_Entrada|Visitantes_Millones|\n",
      "+--------------------+-------+---------+--------------+-------------------+\n",
      "| Great Wall of China|  China|      Yes|            10|               10.0|\n",
      "|           Taj Mahal|  India|      Yes|            15|                7.5|\n",
      "|           Colosseum|  Italy|      Yes|            18|               7.65|\n",
      "|      Forbidden City|  China|      Yes|             8|                9.0|\n",
      "|Notre-Dame Cathedral| France|      Yes|             0|               13.0|\n",
      "+--------------------+-------+---------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fil = df_es.filter(\n",
    "        (col(\"Es_UNESCO\").contains(\"Yes\")) & \n",
    "        (col(\"Precio_Entrada\") <= 20)\n",
    "    )\n",
    "\n",
    "df_fil.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703957db",
   "metadata": {},
   "source": [
    "## Dataset 3: Registro turístico de Castilla y León"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd67c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "|              nombre|                tipo|provincia|                 web|               email|\n",
      "+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "|BERNARDO MORO MEN...|Profesional de Tu...| Asturias|                NULL|bernardomoro@hotm...|\n",
      "|        LA SASTRERÍA|Casa Rural de Alq...|    Ávila|www.lasastreriade...|                NULL|\n",
      "|         LAS HAZANAS|Casa Rural de Alq...|    Ávila|                NULL|lashazanas@hotmai...|\n",
      "| LA CASITA DEL PAJAR|Casa Rural de Alq...|    Ávila|                NULL|lashazanas@hotmai...|\n",
      "|            MARACANA|                 Bar|    Ávila|                NULL|emo123anatoliev@g...|\n",
      "+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_schema = StructType([\n",
    "    StructField(\"establecimiento\", StringType(), True),\n",
    "    StructField(\"n_registro\", StringType(), True),\n",
    "    StructField(\"codigo\", StringType(), True),\n",
    "    StructField(\"tipo\", StringType(), True),\n",
    "    StructField(\"categoria\", StringType(), True),\n",
    "    StructField(\"especialidades\", StringType(), True),\n",
    "    StructField(\"clase\", StringType(), True),\n",
    "    StructField(\"nombre\", StringType(), True),\n",
    "    StructField(\"direccion\", StringType(), True),\n",
    "    StructField(\"c_postal\", StringType(), True), \n",
    "    StructField(\"provincia\", StringType(), True),\n",
    "    StructField(\"municipio\", StringType(), True),\n",
    "    StructField(\"localidad\", StringType(), True),\n",
    "    StructField(\"nucleo\", StringType(), True),\n",
    "    StructField(\"telefono_1\", StringType(), True),\n",
    "    StructField(\"telefono_2\", StringType(), True),\n",
    "    StructField(\"telefono_3\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"web\", StringType(), True),\n",
    "    StructField(\"q_calidad\", StringType(), True),\n",
    "    StructField(\"posada_real\", StringType(), True),\n",
    "    StructField(\"plazas\", IntegerType(), True),\n",
    "    StructField(\"gps_longitud\", FloatType(), True),\n",
    "    StructField(\"gps_latitud\", FloatType(), True),\n",
    "    StructField(\"accesible_a_personas_con_discapacidad\", StringType(), True),\n",
    "    StructField(\"column_27\", StringType(), True),\n",
    "    StructField(\"posicion\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"header\",\"true\")\n",
    "      .schema(main_schema)\n",
    "      .option(\"sep\", \";\")\n",
    "      .load(\"registro-de-turismo-de-castilla-y-leon.csv\"))\n",
    "\n",
    "\n",
    "df_contactos = df.select(\"nombre\", \"tipo\", \"provincia\", \"web\", \"email\")\n",
    "\n",
    "df_contactos.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7d888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------------+---------+--------------------+--------------------+\n",
      "|nombre_establecimiento| categoria_actividad|provincia|           sitio_web|  correo_electronico|\n",
      "+----------------------+--------------------+---------+--------------------+--------------------+\n",
      "|  BERNARDO MORO MEN...|Profesional de Tu...| Asturias|                NULL|bernardomoro@hotm...|\n",
      "|          LA SASTRERÍA|Casa Rural de Alq...|    Ávila|www.lasastreriade...|                NULL|\n",
      "|           LAS HAZANAS|Casa Rural de Alq...|    Ávila|                NULL|lashazanas@hotmai...|\n",
      "|   LA CASITA DEL PAJAR|Casa Rural de Alq...|    Ávila|                NULL|lashazanas@hotmai...|\n",
      "|              MARACANA|                 Bar|    Ávila|                NULL|emo123anatoliev@g...|\n",
      "+----------------------+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_limpio = (df_contactos\n",
    "    .withColumnRenamed(\"nombre\", \"nombre_establecimiento\")\n",
    "    .withColumnRenamed(\"tipo\", \"categoria_actividad\")\n",
    "    .withColumnRenamed(\"web\", \"sitio_web\")\n",
    "    .withColumnRenamed(\"email\", \"correo_electronico\")\n",
    ")\n",
    "\n",
    "df_limpio.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a77991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------------+---------+--------------------+--------------------+\n",
      "|nombre_establecimiento| categoria_actividad|provincia|           sitio_web|  correo_electronico|\n",
      "+----------------------+--------------------+---------+--------------------+--------------------+\n",
      "|        BODEGAS TARSUS|g - Bodegas y los...|   Burgos|  www.tarsusvino.com|                NULL|\n",
      "|  BODEGAS DOMINIO D...|g - Bodegas y los...|   Burgos|www.dominiodecair...|bodegas@dominiode...|\n",
      "|    TERRITORIO LUTHIER|g - Bodegas y los...|   Burgos|territorioluthier...|luthier@territori...|\n",
      "|    BODEGA COVARRUBIAS|g - Bodegas y los...|   Burgos| http://valdable.com|   info@valdable.com|\n",
      "|  BODEGAS PASCUAL, ...|g - Bodegas y los...|   Burgos|222.bodegaspascua...|export@bodegaspas...|\n",
      "|   BODEGAS VINUM VITAE|g - Bodegas y los...|   Burgos|      www.avañate.es|vinum.vitae.bodeg...|\n",
      "|  VIÑEDOS Y BODEGAS...|g - Bodegas y los...|   Burgos|     www.ferratus.es|administracion@fe...|\n",
      "|  BODEGAS Y VIÑEDOS...|g - Bodegas y los...|   Burgos|     www.pradorey.es|   info@pradorey.com|\n",
      "|       BODEGAS ARROCAL|g - Bodegas y los...|   Burgos|     www.arrocal.com|  blanca@arrocal.com|\n",
      "|           VIÑA ARNÁIZ|g - Bodegas y los...|   Burgos|  www.vinaarnaiz.com|   enoturismo@jgc.es|\n",
      "|    BODEGAS MONTE AMÁN|g - Bodegas y los...|   Burgos|   www.monteaman.com|bodegas@monteaman...|\n",
      "|  BODEGAS PALACIO D...|g - Bodegas y los...|   Burgos|www.palaciodelerm...|info@palaciodeler...|\n",
      "|         ALONSO ANGULO|g - Bodegas y los...|   Burgos|www.alonsoangulo.com|info@alonsoangulo...|\n",
      "|  VIÑA MAMBRILLA, S.L.|g - Bodegas y los...|   Burgos|   www.mambrilla.com| bodegamambrilla.com|\n",
      "|  BODEGAS TRASLASCU...|g - Bodegas y los...|   Burgos|www.bodegastrasla...|administracion@bo...|\n",
      "|  BODEGAS RODERO, S.L.|g - Bodegas y los...|   Burgos|www.bodegasrodero...|rodero@bodegasrod...|\n",
      "|  BODEGAS HERMANOS ...|g - Bodegas y los...|   Burgos|www.perezpascuas.com|viñapedrosa@perez...|\n",
      "|    BOSQUE DE MATASNOS|g - Bodegas y los...|   Burgos|https://bosquedem...|administracion@bo...|\n",
      "|  BODEGAS PRADO DE ...|g - Bodegas y los...|   Burgos|www.pradodeolmedo...|pradodeolmedo@pra...|\n",
      "|  VISITAS ENOTURÍST...|g - Bodegas y los...|   Burgos|www.lopezcristoba...|bodega@lopezcrist...|\n",
      "+----------------------+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final = df_limpio.filter(\n",
    "    (col(\"provincia\") == \"Burgos\") & \n",
    "    (col(\"categoria_actividad\").like(\"%Bodegas%\")) & \n",
    "    (col(\"sitio_web\").isNotNull()) & \n",
    "    (col(\"sitio_web\") != \"\")\n",
    ")\n",
    "\n",
    "\n",
    "df_final.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
