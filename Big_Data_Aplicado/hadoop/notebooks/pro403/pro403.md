```python
%cd pro403
```

    [Errno 2] No such file or directory: 'pro403'
    /media/notebooks/pro403


    /usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.
      bkms = self.shell.db.get('bookmarks', {})



```python
!hdfs dfs -mkdir /pro403
```

    ^C



```python
!hdfs dfs -put ./logs.log /pro403
```

    put: `/pro403/logs.log': File exists



```python
%%writefile mapper1.py
#!/usr/bin/env python3

import sys 

for line in sys.stdin:
    line = line.strip().split()
    print(f"{line[8]} {1}")

```

    Overwriting mapper1.py



```python
%%writefile reducer1.py
#!/usr/bin/env python3

import sys 

dic = {}

for line in sys.stdin:
    code , val = line.strip().split(" ",1)
    val = int(val)

    if code not in dic:
        dic[code] = val
    else:
        dic[code] = dic[code] + val


for code  ,val in dic.items():

    print(f"{code} , {val}")



```

    Overwriting reducer1.py



```python
!cat logs.log | python3 mapper1.py | sort | python3 reducer1.py
```

    200 , 15
    303 , 15
    304 , 17
    403 , 8
    404 , 12
    500 , 15
    502 , 18



```python
!hdfs dfs -rm .r /salida/pro403
```

    rm: `.r': No such file or directory
    rm: `/salida/pro403': Is a directory



```python
!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar -file mapper1.py -file reducer1.py -mapper mapper1.py -reducer reducer1.py -input /pro403/logs.log -output /salida/pro403
```

    2025-12-03 17:17:51,802 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.
    packageJobJar: [mapper1.py, reducer1.py, /tmp/hadoop-unjar7218642056655853638/] [] /tmp/streamjob4825616359197957746.jar tmpDir=null
    2025-12-03 17:17:52,428 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032
    2025-12-03 17:17:52,592 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032
    2025-12-03 17:17:52,692 ERROR streaming.StreamJob: Error Launching job : Output directory hdfs://namenode:9000/salida/pro403 already exists
    Streaming Command Failed!



```python
!hdfs dfs -cat /salida/pro403/part-00000
```

    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (0, 1)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "-" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (0, 1)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	
    "http://www.parker-miller.org/tag/list/list/privacy/" , (1, 0)	
    	



```python
%%writefile mapper2.py
#!/usr/bin/env python3

import sys 

for line in sys.stdin:
    line = line.strip().split()
    print(f"{line[0]} {line[9]}")

```

    Overwriting mapper2.py



```python
%%writefile reducer2.py
#!/usr/bin/env python3

import sys 

for line in sys.stdin:
    ip , val = line.strip().split(" ",1)
    bytes = int(val)

    print(f"{ip}: {bytes} bytes")


```

    Overwriting reducer2.py



```python
!hdfs dfs -rm -r /salida/pro403
```

    Deleted /salida/pro403



```python
!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar -file mapper2.py -file reducer2.py -mapper mapper2.py -reducer reducer2.py -input /pro403/logs.log -output /salida/pro403
```

    2025-12-03 17:17:56,728 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.
    packageJobJar: [mapper2.py, reducer2.py, /tmp/hadoop-unjar1603307357110558158/] [] /tmp/streamjob506945606058799496.jar tmpDir=null
    2025-12-03 17:17:57,316 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032
    2025-12-03 17:17:57,439 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032
    2025-12-03 17:17:57,609 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764778124251_0013
    2025-12-03 17:17:58,009 INFO mapred.FileInputFormat: Total input files to process : 1
    2025-12-03 17:17:58,098 INFO mapreduce.JobSubmitter: number of splits:2
    2025-12-03 17:17:58,212 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764778124251_0013
    2025-12-03 17:17:58,213 INFO mapreduce.JobSubmitter: Executing with tokens: []
    2025-12-03 17:17:58,369 INFO conf.Configuration: resource-types.xml not found
    2025-12-03 17:17:58,369 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
    2025-12-03 17:17:58,442 INFO impl.YarnClientImpl: Submitted application application_1764778124251_0013
    2025-12-03 17:17:58,473 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764778124251_0013/
    2025-12-03 17:17:58,474 INFO mapreduce.Job: Running job: job_1764778124251_0013
    2025-12-03 17:18:03,536 INFO mapreduce.Job: Job job_1764778124251_0013 running in uber mode : false
    2025-12-03 17:18:03,538 INFO mapreduce.Job:  map 0% reduce 0%
    2025-12-03 17:18:07,585 INFO mapreduce.Job:  map 100% reduce 0%
    2025-12-03 17:18:11,823 INFO mapreduce.Job:  map 100% reduce 100%
    2025-12-03 17:18:11,831 INFO mapreduce.Job: Job job_1764778124251_0013 completed successfully
    2025-12-03 17:18:11,894 INFO mapreduce.Job: Counters: 54
    	File System Counters
    		FILE: Number of bytes read=2239
    		FILE: Number of bytes written=946831
    		FILE: Number of read operations=0
    		FILE: Number of large read operations=0
    		FILE: Number of write operations=0
    		HDFS: Number of bytes read=28593
    		HDFS: Number of bytes written=2733
    		HDFS: Number of read operations=11
    		HDFS: Number of large read operations=0
    		HDFS: Number of write operations=2
    		HDFS: Number of bytes read erasure-coded=0
    	Job Counters 
    		Launched map tasks=2
    		Launched reduce tasks=1
    		Data-local map tasks=2
    		Total time spent by all maps in occupied slots (ms)=3863
    		Total time spent by all reduces in occupied slots (ms)=1867
    		Total time spent by all map tasks (ms)=3863
    		Total time spent by all reduce tasks (ms)=1867
    		Total vcore-milliseconds taken by all map tasks=3863
    		Total vcore-milliseconds taken by all reduce tasks=1867
    		Total megabyte-milliseconds taken by all map tasks=3955712
    		Total megabyte-milliseconds taken by all reduce tasks=1911808
    	Map-Reduce Framework
    		Map input records=100
    		Map output records=100
    		Map output bytes=2033
    		Map output materialized bytes=2245
    		Input split bytes=176
    		Combine input records=0
    		Combine output records=0
    		Reduce input groups=100
    		Reduce shuffle bytes=2245
    		Reduce input records=100
    		Reduce output records=100
    		Spilled Records=200
    		Shuffled Maps =2
    		Failed Shuffles=0
    		Merged Map outputs=2
    		GC time elapsed (ms)=123
    		CPU time spent (ms)=1800
    		Physical memory (bytes) snapshot=858656768
    		Virtual memory (bytes) snapshot=7779262464
    		Total committed heap usage (bytes)=671088640
    		Peak Map Physical memory (bytes)=356966400
    		Peak Map Virtual memory (bytes)=2591318016
    		Peak Reduce Physical memory (bytes)=257069056
    		Peak Reduce Virtual memory (bytes)=2597216256
    	Shuffle Errors
    		BAD_ID=0
    		CONNECTION=0
    		IO_ERROR=0
    		WRONG_LENGTH=0
    		WRONG_MAP=0
    		WRONG_REDUCE=0
    	File Input Format Counters 
    		Bytes Read=28417
    	File Output Format Counters 
    		Bytes Written=2733
    2025-12-03 17:18:11,894 INFO streaming.StreamJob: Output directory: /salida/pro403



```python
!hdfs dfs -cat /salida/pro403/part-00000
```

    0.89.70.97: 5017 bytes	
    102.107.36.69: 4932 bytes	
    102.247.49.87: 4959 bytes	
    103.176.203.18: 5033 bytes	
    106.15.105.115: 5046 bytes	
    112.164.86.231: 5024 bytes	
    115.101.165.251: 4965 bytes	
    119.170.1.203: 5011 bytes	
    12.131.19.214: 4981 bytes	
    125.87.60.188: 4961 bytes	
    127.178.20.10: 5031 bytes	
    127.72.62.237: 4955 bytes	
    132.176.187.103: 4973 bytes	
    136.170.251.93: 4978 bytes	
    137.196.118.126: 4960 bytes	
    137.205.180.143: 5053 bytes	
    141.46.157.247: 4965 bytes	
    142.19.81.223: 4924 bytes	
    142.20.69.223: 5038 bytes	
    144.140.97.239: 4934 bytes	
    144.85.122.244: 4976 bytes	
    148.5.169.251: 5044 bytes	
    149.194.199.18: 5017 bytes	
    154.131.45.155: 5059 bytes	
    157.44.23.213: 5022 bytes	
    159.238.93.133: 4959 bytes	
    160.36.208.51: 4979 bytes	
    162.253.4.179: 5041 bytes	
    172.172.252.161: 4969 bytes	
    172.175.150.80: 4986 bytes	
    182.215.249.159: 4936 bytes	
    183.129.168.199: 4990 bytes	
    183.155.98.170: 5098 bytes	
    183.51.44.206: 4952 bytes	
    187.13.150.49: 5029 bytes	
    190.247.167.164: 5005 bytes	
    195.47.94.9: 5024 bytes	
    202.8.213.171: 4957 bytes	
    203.40.76.61: 4922 bytes	
    204.34.245.97: 4917 bytes	
    207.194.20.187: 4989 bytes	
    211.216.195.195: 5068 bytes	
    211.69.213.210: 4992 bytes	
    213.127.117.32: 5036 bytes	
    213.90.185.182: 4926 bytes	
    214.60.60.96: 5001 bytes	
    217.20.3.105: 4951 bytes	
    220.194.253.161: 4966 bytes	
    223.254.74.157: 4989 bytes	
    224.219.85.67: 5006 bytes	
    226.208.16.190: 5043 bytes	
    228.181.201.16: 4936 bytes	
    23.162.51.3: 4940 bytes	
    230.230.213.17: 5024 bytes	
    233.200.137.100: 4965 bytes	
    233.223.117.90: 4963 bytes	
    233.228.7.148: 5066 bytes	
    236.192.181.134: 4995 bytes	
    238.204.144.175: 5056 bytes	
    238.217.83.154: 5152 bytes	
    242.114.247.252: 5077 bytes	
    247.61.75.27: 4931 bytes	
    25.254.82.147: 5067 bytes	
    250.68.88.150: 4972 bytes	
    252.156.232.172: 5028 bytes	
    254.54.186.144: 5044 bytes	
    255.231.52.33: 5054 bytes	
    255.27.84.112: 5089 bytes	
    26.167.128.186: 5060 bytes	
    26.44.136.193: 4925 bytes	
    40.129.224.59: 5023 bytes	
    41.193.26.139: 4917 bytes	
    41.204.111.51: 5044 bytes	
    46.141.41.90: 4981 bytes	
    49.144.29.85: 5029 bytes	
    51.111.178.196: 4973 bytes	
    52.135.10.160: 4995 bytes	
    55.25.7.93: 5016 bytes	
    58.30.103.184: 5024 bytes	
    59.107.116.6: 5008 bytes	
    6.110.39.101: 4947 bytes	
    65.104.60.28: 5084 bytes	
    66.181.188.94: 4966 bytes	
    66.221.171.61: 5034 bytes	
    68.186.225.176: 5101 bytes	
    68.226.160.154: 5035 bytes	
    71.158.198.139: 5077 bytes	
    71.8.223.77: 4950 bytes	
    73.91.150.125: 5028 bytes	
    76.54.177.164: 4969 bytes	
    78.88.250.115: 4986 bytes	
    81.23.229.106: 5019 bytes	
    84.89.201.191: 4997 bytes	
    85.2.215.227: 4980 bytes	
    86.194.222.239: 4881 bytes	
    90.64.62.239: 4947 bytes	
    92.213.35.47: 4997 bytes	
    93.234.0.101: 5088 bytes	
    93.47.162.191: 4967 bytes	
    97.15.37.214: 4980 bytes	



```python
%%writefile mapper3.py
#!/usr/bin/env python3

import sys

for line in sys.stdin:

    line = line.strip().split()
    print(f"{line[11]}, 1")
```

    Overwriting mapper3.py



```python
%%writefile reducer3.py
#!/usr/bin/env python3


import sys 

dic = {}

for line in sys.stdin:

    url , val = line.strip().split(",",1)
    val = int(val)

    if url not in dic:
        dic[url] = val  
    else: 
        dic[url] = dic[url] + val

for url , val in dic.items():
    print(f"{url} , {val}")
```

    Overwriting reducer3.py



```python
!hdfs dfs -rm -r /salida/pro403
```

    Deleted /salida/pro403



```python
!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar -file mapper3.py -file reducer3.py -mapper mapper3.py -reducer reducer3.py -input /pro403/logs.log -output /salida/pro403
```

    2025-12-03 17:18:15,908 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.
    packageJobJar: [mapper3.py, reducer3.py, /tmp/hadoop-unjar4265493501250514854/] [] /tmp/streamjob1995712296196752453.jar tmpDir=null
    2025-12-03 17:18:16,495 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032
    2025-12-03 17:18:16,605 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032
    2025-12-03 17:18:16,797 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764778124251_0014
    2025-12-03 17:18:17,157 INFO mapred.FileInputFormat: Total input files to process : 1
    2025-12-03 17:18:17,231 INFO mapreduce.JobSubmitter: number of splits:2
    2025-12-03 17:18:17,337 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764778124251_0014
    2025-12-03 17:18:17,337 INFO mapreduce.JobSubmitter: Executing with tokens: []
    2025-12-03 17:18:17,478 INFO conf.Configuration: resource-types.xml not found
    2025-12-03 17:18:17,478 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
    2025-12-03 17:18:17,530 INFO impl.YarnClientImpl: Submitted application application_1764778124251_0014
    2025-12-03 17:18:17,554 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764778124251_0014/
    2025-12-03 17:18:17,554 INFO mapreduce.Job: Running job: job_1764778124251_0014
    2025-12-03 17:18:21,650 INFO mapreduce.Job: Job job_1764778124251_0014 running in uber mode : false
    2025-12-03 17:18:21,651 INFO mapreduce.Job:  map 0% reduce 0%
    2025-12-03 17:18:25,743 INFO mapreduce.Job:  map 100% reduce 0%
    2025-12-03 17:18:30,767 INFO mapreduce.Job:  map 100% reduce 100%
    2025-12-03 17:18:30,775 INFO mapreduce.Job: Job job_1764778124251_0014 completed successfully
    2025-12-03 17:18:30,836 INFO mapreduce.Job: Counters: 54
    	File System Counters
    		FILE: Number of bytes read=1906
    		FILE: Number of bytes written=946168
    		FILE: Number of read operations=0
    		FILE: Number of large read operations=0
    		FILE: Number of write operations=0
    		HDFS: Number of bytes read=28593
    		HDFS: Number of bytes written=20
    		HDFS: Number of read operations=11
    		HDFS: Number of large read operations=0
    		HDFS: Number of write operations=2
    		HDFS: Number of bytes read erasure-coded=0
    	Job Counters 
    		Launched map tasks=2
    		Launched reduce tasks=1
    		Data-local map tasks=2
    		Total time spent by all maps in occupied slots (ms)=3830
    		Total time spent by all reduces in occupied slots (ms)=1610
    		Total time spent by all map tasks (ms)=3830
    		Total time spent by all reduce tasks (ms)=1610
    		Total vcore-milliseconds taken by all map tasks=3830
    		Total vcore-milliseconds taken by all reduce tasks=1610
    		Total megabyte-milliseconds taken by all map tasks=3921920
    		Total megabyte-milliseconds taken by all reduce tasks=1648640
    	Map-Reduce Framework
    		Map input records=100
    		Map output records=100
    		Map output bytes=1700
    		Map output materialized bytes=1912
    		Input split bytes=176
    		Combine input records=0
    		Combine output records=0
    		Reduce input groups=1
    		Reduce shuffle bytes=1912
    		Reduce input records=100
    		Reduce output records=1
    		Spilled Records=200
    		Shuffled Maps =2
    		Failed Shuffles=0
    		Merged Map outputs=2
    		GC time elapsed (ms)=153
    		CPU time spent (ms)=1710
    		Physical memory (bytes) snapshot=975175680
    		Virtual memory (bytes) snapshot=7782219776
    		Total committed heap usage (bytes)=783810560
    		Peak Map Physical memory (bytes)=357740544
    		Peak Map Virtual memory (bytes)=2591580160
    		Peak Reduce Physical memory (bytes)=260190208
    		Peak Reduce Virtual memory (bytes)=2599448576
    	Shuffle Errors
    		BAD_ID=0
    		CONNECTION=0
    		IO_ERROR=0
    		WRONG_LENGTH=0
    		WRONG_MAP=0
    		WRONG_REDUCE=0
    	File Input Format Counters 
    		Bytes Read=28417
    	File Output Format Counters 
    		Bytes Written=20
    2025-12-03 17:18:30,836 INFO streaming.StreamJob: Output directory: /salida/pro403



```python
!hdfs dfs -cat /salida/pro403/part-00000
```

    "Mozilla/5.0 , 100	



```python
%%writefile mapper4.py
#!/usr/bin/env python3

import sys

for line in sys.stdin:

    line = line.strip().split()
    print(f"{line[5]}, 1")
```

    Overwriting mapper4.py



```python
%%writefile reducer4.py
#!/usr/bin/env python3


import sys 

dic = {}

for line in sys.stdin:

    metodo , val = line.strip().split(",",1)
    val = int(val)

    if metodo not in dic:
        dic[metodo] = val  
    else: 
        dic[metodo] = dic[metodo] + val

for metodo , val in dic.items():
    print(f"{metodo} , {val}")
```

    Overwriting reducer4.py



```python
!hdfs dfs -rm -r /salida/pro403
```

    Deleted /salida/pro403



```python
!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar -file mapper4.py -file reducer4.py -mapper mapper4.py -reducer reducer4.py -input /pro403/logs.log -output /salida/pro403
```

    2025-12-03 17:18:34,876 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.
    packageJobJar: [mapper4.py, reducer4.py, /tmp/hadoop-unjar3806706490349814775/] [] /tmp/streamjob1109680042581956835.jar tmpDir=null
    2025-12-03 17:18:35,486 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032
    2025-12-03 17:18:35,595 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032
    2025-12-03 17:18:35,764 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764778124251_0015
    2025-12-03 17:18:36,106 INFO mapred.FileInputFormat: Total input files to process : 1
    2025-12-03 17:18:36,182 INFO mapreduce.JobSubmitter: number of splits:2
    2025-12-03 17:18:36,295 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764778124251_0015
    2025-12-03 17:18:36,295 INFO mapreduce.JobSubmitter: Executing with tokens: []
    2025-12-03 17:18:36,459 INFO conf.Configuration: resource-types.xml not found
    2025-12-03 17:18:36,459 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
    2025-12-03 17:18:36,514 INFO impl.YarnClientImpl: Submitted application application_1764778124251_0015
    2025-12-03 17:18:36,551 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764778124251_0015/
    2025-12-03 17:18:36,554 INFO mapreduce.Job: Running job: job_1764778124251_0015
    2025-12-03 17:18:40,641 INFO mapreduce.Job: Job job_1764778124251_0015 running in uber mode : false
    2025-12-03 17:18:40,642 INFO mapreduce.Job:  map 0% reduce 0%
    2025-12-03 17:18:45,931 INFO mapreduce.Job:  map 100% reduce 0%
    2025-12-03 17:18:49,954 INFO mapreduce.Job:  map 100% reduce 100%
    2025-12-03 17:18:49,961 INFO mapreduce.Job: Job job_1764778124251_0015 completed successfully
    2025-12-03 17:18:50,021 INFO mapreduce.Job: Counters: 54
    	File System Counters
    		FILE: Number of bytes read=1199
    		FILE: Number of bytes written=944754
    		FILE: Number of read operations=0
    		FILE: Number of large read operations=0
    		FILE: Number of write operations=0
    		HDFS: Number of bytes read=28593
    		HDFS: Number of bytes written=48
    		HDFS: Number of read operations=11
    		HDFS: Number of large read operations=0
    		HDFS: Number of write operations=2
    		HDFS: Number of bytes read erasure-coded=0
    	Job Counters 
    		Launched map tasks=2
    		Launched reduce tasks=1
    		Data-local map tasks=2
    		Total time spent by all maps in occupied slots (ms)=4036
    		Total time spent by all reduces in occupied slots (ms)=1807
    		Total time spent by all map tasks (ms)=4036
    		Total time spent by all reduce tasks (ms)=1807
    		Total vcore-milliseconds taken by all map tasks=4036
    		Total vcore-milliseconds taken by all reduce tasks=1807
    		Total megabyte-milliseconds taken by all map tasks=4132864
    		Total megabyte-milliseconds taken by all reduce tasks=1850368
    	Map-Reduce Framework
    		Map input records=100
    		Map output records=100
    		Map output bytes=993
    		Map output materialized bytes=1205
    		Input split bytes=176
    		Combine input records=0
    		Combine output records=0
    		Reduce input groups=4
    		Reduce shuffle bytes=1205
    		Reduce input records=100
    		Reduce output records=4
    		Spilled Records=200
    		Shuffled Maps =2
    		Failed Shuffles=0
    		Merged Map outputs=2
    		GC time elapsed (ms)=141
    		CPU time spent (ms)=1720
    		Physical memory (bytes) snapshot=970838016
    		Virtual memory (bytes) snapshot=7778439168
    		Total committed heap usage (bytes)=781713408
    		Peak Map Physical memory (bytes)=360452096
    		Peak Map Virtual memory (bytes)=2590748672
    		Peak Reduce Physical memory (bytes)=252846080
    		Peak Reduce Virtual memory (bytes)=2597605376
    	Shuffle Errors
    		BAD_ID=0
    		CONNECTION=0
    		IO_ERROR=0
    		WRONG_LENGTH=0
    		WRONG_MAP=0
    		WRONG_REDUCE=0
    	File Input Format Counters 
    		Bytes Read=28417
    	File Output Format Counters 
    		Bytes Written=48
    2025-12-03 17:18:50,021 INFO streaming.StreamJob: Output directory: /salida/pro403



```python
!hdfs dfs -cat /salida/pro403/part-00000
```

    "DELETE , 23	
    "GET , 25	
    "POST , 24	
    "PUT , 28	



```python
%%writefile mapper5.py
#!/usr/bin/env python3

import sys

for line in sys.stdin:

    line = line.strip().split()

    fecha = line[3]

    hora = fecha[13:15]
    
   

    print(f"{hora}, 1")



    
```

    Overwriting mapper5.py



```python
%%writefile reducer5.py
#!/usr/bin/env python3


import sys 

dic = {}

for line in sys.stdin:

    hora , val = line.strip().split(",",1)
    val = int(val)


    if hora not in dic:
        dic[hora] = val  
    else: 
        dic[hora] = dic[hora] + val

for hora , val in dic.items():
    print(f"{hora} , {val}")
```

    Overwriting reducer5.py



```python
!hdfs dfs -rm -r /salida/pro403
```

    Deleted /salida/pro403



```python
!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar -file mapper5.py -file reducer5.py -mapper mapper5.py -reducer reducer5.py -input /pro403/logs.log -output /salida/pro403
```

    2025-12-03 17:18:54,163 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.
    packageJobJar: [mapper5.py, reducer5.py, /tmp/hadoop-unjar4361864957979567555/] [] /tmp/streamjob6769994024710750708.jar tmpDir=null
    2025-12-03 17:18:54,791 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032
    2025-12-03 17:18:54,914 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032
    2025-12-03 17:18:55,082 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764778124251_0016
    2025-12-03 17:18:55,411 INFO mapred.FileInputFormat: Total input files to process : 1
    2025-12-03 17:18:55,478 INFO mapreduce.JobSubmitter: number of splits:2
    2025-12-03 17:18:55,592 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764778124251_0016
    2025-12-03 17:18:55,592 INFO mapreduce.JobSubmitter: Executing with tokens: []
    2025-12-03 17:18:55,782 INFO conf.Configuration: resource-types.xml not found
    2025-12-03 17:18:55,782 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
    2025-12-03 17:18:55,848 INFO impl.YarnClientImpl: Submitted application application_1764778124251_0016
    2025-12-03 17:18:55,881 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764778124251_0016/
    2025-12-03 17:18:55,882 INFO mapreduce.Job: Running job: job_1764778124251_0016
    2025-12-03 17:18:59,969 INFO mapreduce.Job: Job job_1764778124251_0016 running in uber mode : false
    2025-12-03 17:18:59,970 INFO mapreduce.Job:  map 0% reduce 0%
    2025-12-03 17:19:05,066 INFO mapreduce.Job:  map 100% reduce 0%
    2025-12-03 17:19:09,089 INFO mapreduce.Job:  map 100% reduce 100%
    2025-12-03 17:19:09,098 INFO mapreduce.Job: Job job_1764778124251_0016 completed successfully
    2025-12-03 17:19:09,196 INFO mapreduce.Job: Counters: 54
    	File System Counters
    		FILE: Number of bytes read=906
    		FILE: Number of bytes written=944168
    		FILE: Number of read operations=0
    		FILE: Number of large read operations=0
    		FILE: Number of write operations=0
    		HDFS: Number of bytes read=28593
    		HDFS: Number of bytes written=10
    		HDFS: Number of read operations=11
    		HDFS: Number of large read operations=0
    		HDFS: Number of write operations=2
    		HDFS: Number of bytes read erasure-coded=0
    	Job Counters 
    		Launched map tasks=2
    		Launched reduce tasks=1
    		Data-local map tasks=2
    		Total time spent by all maps in occupied slots (ms)=4115
    		Total time spent by all reduces in occupied slots (ms)=1658
    		Total time spent by all map tasks (ms)=4115
    		Total time spent by all reduce tasks (ms)=1658
    		Total vcore-milliseconds taken by all map tasks=4115
    		Total vcore-milliseconds taken by all reduce tasks=1658
    		Total megabyte-milliseconds taken by all map tasks=4213760
    		Total megabyte-milliseconds taken by all reduce tasks=1697792
    	Map-Reduce Framework
    		Map input records=100
    		Map output records=100
    		Map output bytes=700
    		Map output materialized bytes=912
    		Input split bytes=176
    		Combine input records=0
    		Combine output records=0
    		Reduce input groups=1
    		Reduce shuffle bytes=912
    		Reduce input records=100
    		Reduce output records=1
    		Spilled Records=200
    		Shuffled Maps =2
    		Failed Shuffles=0
    		Merged Map outputs=2
    		GC time elapsed (ms)=151
    		CPU time spent (ms)=1960
    		Physical memory (bytes) snapshot=954535936
    		Virtual memory (bytes) snapshot=7781597184
    		Total committed heap usage (bytes)=773324800
    		Peak Map Physical memory (bytes)=360472576
    		Peak Map Virtual memory (bytes)=2592256000
    		Peak Reduce Physical memory (bytes)=253636608
    		Peak Reduce Virtual memory (bytes)=2598174720
    	Shuffle Errors
    		BAD_ID=0
    		CONNECTION=0
    		IO_ERROR=0
    		WRONG_LENGTH=0
    		WRONG_MAP=0
    		WRONG_REDUCE=0
    	File Input Format Counters 
    		Bytes Read=28417
    	File Output Format Counters 
    		Bytes Written=10
    2025-12-03 17:19:09,196 INFO streaming.StreamJob: Output directory: /salida/pro403



```python
!hdfs dfs -cat /salida/pro403/part-00000
```

    12 , 100	



```python
%%writefile mapper6.py
#!/usr/bin/env python3

import sys

for line in sys.stdin:

    line = line.strip().split()

    code = int(line[8])

    if code >= 400:
        print(f"{line[10]} , ({0}, {1})")
    else:
        print(f"{line[10]} , ({1}, {0})")


```

    Overwriting mapper6.py



```python
%%writefile reducer6.py
#!/usr/bin/env python3


import sys 

total = 0
sum = 0 

for line in sys.stdin:

    url , code = line.strip().split(",",1)
    code = code.strip()
    if code == "(1, 0)":
        sum = sum + 1
    total = total + 1


porcentaje = (sum / total) * 100

print(f"El porcentaje de errores es {porcentaje}%")
```

    Overwriting reducer6.py



```python
!hdfs dfs -rm -r /salida/pro403
```

    Deleted /salida/pro403



```python
!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar -file mapper6.py -file reducer6.py -mapper mapper6.py -reducer reducer6.py -input /pro403/logs.log -output /salida/pro403
```

    2025-12-03 17:39:41,270 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.
    packageJobJar: [mapper6.py, reducer6.py, /tmp/hadoop-unjar3642688062347268694/] [] /tmp/streamjob5174099571239973979.jar tmpDir=null
    2025-12-03 17:39:41,817 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032
    2025-12-03 17:39:41,942 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032
    2025-12-03 17:39:42,116 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764778124251_0026
    2025-12-03 17:39:42,456 INFO mapred.FileInputFormat: Total input files to process : 1
    2025-12-03 17:39:42,537 INFO mapreduce.JobSubmitter: number of splits:2
    2025-12-03 17:39:42,627 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764778124251_0026
    2025-12-03 17:39:42,627 INFO mapreduce.JobSubmitter: Executing with tokens: []
    2025-12-03 17:39:42,778 INFO conf.Configuration: resource-types.xml not found
    2025-12-03 17:39:42,779 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
    2025-12-03 17:39:42,841 INFO impl.YarnClientImpl: Submitted application application_1764778124251_0026
    2025-12-03 17:39:42,880 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764778124251_0026/
    2025-12-03 17:39:42,881 INFO mapreduce.Job: Running job: job_1764778124251_0026
    2025-12-03 17:39:47,953 INFO mapreduce.Job: Job job_1764778124251_0026 running in uber mode : false
    2025-12-03 17:39:47,954 INFO mapreduce.Job:  map 0% reduce 0%
    2025-12-03 17:39:52,214 INFO mapreduce.Job:  map 100% reduce 0%
    2025-12-03 17:39:56,240 INFO mapreduce.Job:  map 100% reduce 100%
    2025-12-03 17:39:56,248 INFO mapreduce.Job: Job job_1764778124251_0026 completed successfully
    2025-12-03 17:39:56,319 INFO mapreduce.Job: Counters: 54
    	File System Counters
    		FILE: Number of bytes read=4156
    		FILE: Number of bytes written=950668
    		FILE: Number of read operations=0
    		FILE: Number of large read operations=0
    		FILE: Number of write operations=0
    		HDFS: Number of bytes read=28593
    		HDFS: Number of bytes written=35
    		HDFS: Number of read operations=11
    		HDFS: Number of large read operations=0
    		HDFS: Number of write operations=2
    		HDFS: Number of bytes read erasure-coded=0
    	Job Counters 
    		Launched map tasks=2
    		Launched reduce tasks=1
    		Data-local map tasks=2
    		Total time spent by all maps in occupied slots (ms)=4058
    		Total time spent by all reduces in occupied slots (ms)=1698
    		Total time spent by all map tasks (ms)=4058
    		Total time spent by all reduce tasks (ms)=1698
    		Total vcore-milliseconds taken by all map tasks=4058
    		Total vcore-milliseconds taken by all reduce tasks=1698
    		Total megabyte-milliseconds taken by all map tasks=4155392
    		Total megabyte-milliseconds taken by all reduce tasks=1738752
    	Map-Reduce Framework
    		Map input records=100
    		Map output records=100
    		Map output bytes=3950
    		Map output materialized bytes=4162
    		Input split bytes=176
    		Combine input records=0
    		Combine output records=0
    		Reduce input groups=4
    		Reduce shuffle bytes=4162
    		Reduce input records=100
    		Reduce output records=1
    		Spilled Records=200
    		Shuffled Maps =2
    		Failed Shuffles=0
    		Merged Map outputs=2
    		GC time elapsed (ms)=153
    		CPU time spent (ms)=1930
    		Physical memory (bytes) snapshot=860749824
    		Virtual memory (bytes) snapshot=7781117952
    		Total committed heap usage (bytes)=668991488
    		Peak Map Physical memory (bytes)=359546880
    		Peak Map Virtual memory (bytes)=2595454976
    		Peak Reduce Physical memory (bytes)=252354560
    		Peak Reduce Virtual memory (bytes)=2594689024
    	Shuffle Errors
    		BAD_ID=0
    		CONNECTION=0
    		IO_ERROR=0
    		WRONG_LENGTH=0
    		WRONG_MAP=0
    		WRONG_REDUCE=0
    	File Input Format Counters 
    		Bytes Read=28417
    	File Output Format Counters 
    		Bytes Written=35
    2025-12-03 17:39:56,320 INFO streaming.StreamJob: Output directory: /salida/pro403



```python
!hdfs dfs -cat /salida/pro403/part-00000
```

    El porcentaje de errores es 47.0%	

