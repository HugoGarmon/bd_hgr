{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6064bcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'pro403'\n",
      "/media/notebooks/pro403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "%cd pro403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3ea0d57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /pro403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca2e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/pro403/logs.log': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put ./logs.log /pro403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c4a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys \n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split()\n",
    "    print(f\"{line[8]} {1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d64b097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys \n",
    "\n",
    "dic = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    code , val = line.strip().split(\" \",1)\n",
    "    val = int(val)\n",
    "\n",
    "    if code not in dic:\n",
    "        dic[code] = val\n",
    "    else:\n",
    "        dic[code] = dic[code] + val\n",
    "\n",
    "\n",
    "for code  ,val in dic.items():\n",
    "\n",
    "    print(f\"{code} , {val}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba09562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 , 15\n",
      "303 , 15\n",
      "304 , 17\n",
      "403 , 8\n",
      "404 , 12\n",
      "500 , 15\n",
      "502 , 18\n"
     ]
    }
   ],
   "source": [
    "!cat logs.log | python3 mapper1.py | sort | python3 reducer1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa408a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `.r': No such file or directory\n",
      "rm: `/salida/pro403': Is a directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm .r /salida/pro403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5ad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-03 17:17:51,802 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper1.py, reducer1.py, /tmp/hadoop-unjar7218642056655853638/] [] /tmp/streamjob4825616359197957746.jar tmpDir=null\n",
      "2025-12-03 17:17:52,428 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 17:17:52,592 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 17:17:52,692 ERROR streaming.StreamJob: Error Launching job : Output directory hdfs://namenode:9000/salida/pro403 already exists\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar -file mapper1.py -file reducer1.py -mapper mapper1.py -reducer reducer1.py -input /pro403/logs.log -output /salida/pro403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e54b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (0, 1)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"-\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (0, 1)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n",
      "\"http://www.parker-miller.org/tag/list/list/privacy/\" , (1, 0)\t\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /salida/pro403/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e876e7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys \n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split()\n",
    "    print(f\"{line[0]} {line[9]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc255e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys \n",
    "\n",
    "for line in sys.stdin:\n",
    "    ip , val = line.strip().split(\" \",1)\n",
    "    bytes = int(val)\n",
    "\n",
    "    print(f\"{ip}: {bytes} bytes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dafd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /salida/pro403\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /salida/pro403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265fdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-03 17:17:56,728 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper2.py, reducer2.py, /tmp/hadoop-unjar1603307357110558158/] [] /tmp/streamjob506945606058799496.jar tmpDir=null\n",
      "2025-12-03 17:17:57,316 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 17:17:57,439 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 17:17:57,609 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764778124251_0013\n",
      "2025-12-03 17:17:58,009 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-03 17:17:58,098 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-03 17:17:58,212 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764778124251_0013\n",
      "2025-12-03 17:17:58,213 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-03 17:17:58,369 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-03 17:17:58,369 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-03 17:17:58,442 INFO impl.YarnClientImpl: Submitted application application_1764778124251_0013\n",
      "2025-12-03 17:17:58,473 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764778124251_0013/\n",
      "2025-12-03 17:17:58,474 INFO mapreduce.Job: Running job: job_1764778124251_0013\n",
      "2025-12-03 17:18:03,536 INFO mapreduce.Job: Job job_1764778124251_0013 running in uber mode : false\n",
      "2025-12-03 17:18:03,538 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-03 17:18:07,585 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-03 17:18:11,823 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-03 17:18:11,831 INFO mapreduce.Job: Job job_1764778124251_0013 completed successfully\n",
      "2025-12-03 17:18:11,894 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2239\n",
      "\t\tFILE: Number of bytes written=946831\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=28593\n",
      "\t\tHDFS: Number of bytes written=2733\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3863\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1867\n",
      "\t\tTotal time spent by all map tasks (ms)=3863\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1867\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3863\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1867\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3955712\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1911808\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=2033\n",
      "\t\tMap output materialized bytes=2245\n",
      "\t\tInput split bytes=176\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=2245\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=100\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=123\n",
      "\t\tCPU time spent (ms)=1800\n",
      "\t\tPhysical memory (bytes) snapshot=858656768\n",
      "\t\tVirtual memory (bytes) snapshot=7779262464\n",
      "\t\tTotal committed heap usage (bytes)=671088640\n",
      "\t\tPeak Map Physical memory (bytes)=356966400\n",
      "\t\tPeak Map Virtual memory (bytes)=2591318016\n",
      "\t\tPeak Reduce Physical memory (bytes)=257069056\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2597216256\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=28417\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2733\n",
      "2025-12-03 17:18:11,894 INFO streaming.StreamJob: Output directory: /salida/pro403\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar -file mapper2.py -file reducer2.py -mapper mapper2.py -reducer reducer2.py -input /pro403/logs.log -output /salida/pro403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0723b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89.70.97: 5017 bytes\t\n",
      "102.107.36.69: 4932 bytes\t\n",
      "102.247.49.87: 4959 bytes\t\n",
      "103.176.203.18: 5033 bytes\t\n",
      "106.15.105.115: 5046 bytes\t\n",
      "112.164.86.231: 5024 bytes\t\n",
      "115.101.165.251: 4965 bytes\t\n",
      "119.170.1.203: 5011 bytes\t\n",
      "12.131.19.214: 4981 bytes\t\n",
      "125.87.60.188: 4961 bytes\t\n",
      "127.178.20.10: 5031 bytes\t\n",
      "127.72.62.237: 4955 bytes\t\n",
      "132.176.187.103: 4973 bytes\t\n",
      "136.170.251.93: 4978 bytes\t\n",
      "137.196.118.126: 4960 bytes\t\n",
      "137.205.180.143: 5053 bytes\t\n",
      "141.46.157.247: 4965 bytes\t\n",
      "142.19.81.223: 4924 bytes\t\n",
      "142.20.69.223: 5038 bytes\t\n",
      "144.140.97.239: 4934 bytes\t\n",
      "144.85.122.244: 4976 bytes\t\n",
      "148.5.169.251: 5044 bytes\t\n",
      "149.194.199.18: 5017 bytes\t\n",
      "154.131.45.155: 5059 bytes\t\n",
      "157.44.23.213: 5022 bytes\t\n",
      "159.238.93.133: 4959 bytes\t\n",
      "160.36.208.51: 4979 bytes\t\n",
      "162.253.4.179: 5041 bytes\t\n",
      "172.172.252.161: 4969 bytes\t\n",
      "172.175.150.80: 4986 bytes\t\n",
      "182.215.249.159: 4936 bytes\t\n",
      "183.129.168.199: 4990 bytes\t\n",
      "183.155.98.170: 5098 bytes\t\n",
      "183.51.44.206: 4952 bytes\t\n",
      "187.13.150.49: 5029 bytes\t\n",
      "190.247.167.164: 5005 bytes\t\n",
      "195.47.94.9: 5024 bytes\t\n",
      "202.8.213.171: 4957 bytes\t\n",
      "203.40.76.61: 4922 bytes\t\n",
      "204.34.245.97: 4917 bytes\t\n",
      "207.194.20.187: 4989 bytes\t\n",
      "211.216.195.195: 5068 bytes\t\n",
      "211.69.213.210: 4992 bytes\t\n",
      "213.127.117.32: 5036 bytes\t\n",
      "213.90.185.182: 4926 bytes\t\n",
      "214.60.60.96: 5001 bytes\t\n",
      "217.20.3.105: 4951 bytes\t\n",
      "220.194.253.161: 4966 bytes\t\n",
      "223.254.74.157: 4989 bytes\t\n",
      "224.219.85.67: 5006 bytes\t\n",
      "226.208.16.190: 5043 bytes\t\n",
      "228.181.201.16: 4936 bytes\t\n",
      "23.162.51.3: 4940 bytes\t\n",
      "230.230.213.17: 5024 bytes\t\n",
      "233.200.137.100: 4965 bytes\t\n",
      "233.223.117.90: 4963 bytes\t\n",
      "233.228.7.148: 5066 bytes\t\n",
      "236.192.181.134: 4995 bytes\t\n",
      "238.204.144.175: 5056 bytes\t\n",
      "238.217.83.154: 5152 bytes\t\n",
      "242.114.247.252: 5077 bytes\t\n",
      "247.61.75.27: 4931 bytes\t\n",
      "25.254.82.147: 5067 bytes\t\n",
      "250.68.88.150: 4972 bytes\t\n",
      "252.156.232.172: 5028 bytes\t\n",
      "254.54.186.144: 5044 bytes\t\n",
      "255.231.52.33: 5054 bytes\t\n",
      "255.27.84.112: 5089 bytes\t\n",
      "26.167.128.186: 5060 bytes\t\n",
      "26.44.136.193: 4925 bytes\t\n",
      "40.129.224.59: 5023 bytes\t\n",
      "41.193.26.139: 4917 bytes\t\n",
      "41.204.111.51: 5044 bytes\t\n",
      "46.141.41.90: 4981 bytes\t\n",
      "49.144.29.85: 5029 bytes\t\n",
      "51.111.178.196: 4973 bytes\t\n",
      "52.135.10.160: 4995 bytes\t\n",
      "55.25.7.93: 5016 bytes\t\n",
      "58.30.103.184: 5024 bytes\t\n",
      "59.107.116.6: 5008 bytes\t\n",
      "6.110.39.101: 4947 bytes\t\n",
      "65.104.60.28: 5084 bytes\t\n",
      "66.181.188.94: 4966 bytes\t\n",
      "66.221.171.61: 5034 bytes\t\n",
      "68.186.225.176: 5101 bytes\t\n",
      "68.226.160.154: 5035 bytes\t\n",
      "71.158.198.139: 5077 bytes\t\n",
      "71.8.223.77: 4950 bytes\t\n",
      "73.91.150.125: 5028 bytes\t\n",
      "76.54.177.164: 4969 bytes\t\n",
      "78.88.250.115: 4986 bytes\t\n",
      "81.23.229.106: 5019 bytes\t\n",
      "84.89.201.191: 4997 bytes\t\n",
      "85.2.215.227: 4980 bytes\t\n",
      "86.194.222.239: 4881 bytes\t\n",
      "90.64.62.239: 4947 bytes\t\n",
      "92.213.35.47: 4997 bytes\t\n",
      "93.234.0.101: 5088 bytes\t\n",
      "93.47.162.191: 4967 bytes\t\n",
      "97.15.37.214: 4980 bytes\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /salida/pro403/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63313e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "\n",
    "    line = line.strip().split()\n",
    "    print(f\"{line[11]}, 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5894941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "import sys \n",
    "\n",
    "dic = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "\n",
    "    url , val = line.strip().split(\",\",1)\n",
    "    val = int(val)\n",
    "\n",
    "    if url not in dic:\n",
    "        dic[url] = val  \n",
    "    else: \n",
    "        dic[url] = dic[url] + val\n",
    "\n",
    "for url , val in dic.items():\n",
    "    print(f\"{url} , {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e2070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /salida/pro403\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /salida/pro403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5812f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-03 17:18:15,908 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper3.py, reducer3.py, /tmp/hadoop-unjar4265493501250514854/] [] /tmp/streamjob1995712296196752453.jar tmpDir=null\n",
      "2025-12-03 17:18:16,495 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 17:18:16,605 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 17:18:16,797 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764778124251_0014\n",
      "2025-12-03 17:18:17,157 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-03 17:18:17,231 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-03 17:18:17,337 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764778124251_0014\n",
      "2025-12-03 17:18:17,337 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-03 17:18:17,478 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-03 17:18:17,478 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-03 17:18:17,530 INFO impl.YarnClientImpl: Submitted application application_1764778124251_0014\n",
      "2025-12-03 17:18:17,554 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764778124251_0014/\n",
      "2025-12-03 17:18:17,554 INFO mapreduce.Job: Running job: job_1764778124251_0014\n",
      "2025-12-03 17:18:21,650 INFO mapreduce.Job: Job job_1764778124251_0014 running in uber mode : false\n",
      "2025-12-03 17:18:21,651 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-03 17:18:25,743 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-03 17:18:30,767 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-03 17:18:30,775 INFO mapreduce.Job: Job job_1764778124251_0014 completed successfully\n",
      "2025-12-03 17:18:30,836 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1906\n",
      "\t\tFILE: Number of bytes written=946168\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=28593\n",
      "\t\tHDFS: Number of bytes written=20\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3830\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1610\n",
      "\t\tTotal time spent by all map tasks (ms)=3830\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1610\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3830\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1610\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3921920\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1648640\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=1700\n",
      "\t\tMap output materialized bytes=1912\n",
      "\t\tInput split bytes=176\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=1912\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=153\n",
      "\t\tCPU time spent (ms)=1710\n",
      "\t\tPhysical memory (bytes) snapshot=975175680\n",
      "\t\tVirtual memory (bytes) snapshot=7782219776\n",
      "\t\tTotal committed heap usage (bytes)=783810560\n",
      "\t\tPeak Map Physical memory (bytes)=357740544\n",
      "\t\tPeak Map Virtual memory (bytes)=2591580160\n",
      "\t\tPeak Reduce Physical memory (bytes)=260190208\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2599448576\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=28417\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=20\n",
      "2025-12-03 17:18:30,836 INFO streaming.StreamJob: Output directory: /salida/pro403\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar -file mapper3.py -file reducer3.py -mapper mapper3.py -reducer reducer3.py -input /pro403/logs.log -output /salida/pro403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1ae71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Mozilla/5.0 , 100\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /salida/pro403/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8430ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper4.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "\n",
    "    line = line.strip().split()\n",
    "    print(f\"{line[5]}, 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab6b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer4.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "import sys \n",
    "\n",
    "dic = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "\n",
    "    metodo , val = line.strip().split(\",\",1)\n",
    "    val = int(val)\n",
    "\n",
    "    if metodo not in dic:\n",
    "        dic[metodo] = val  \n",
    "    else: \n",
    "        dic[metodo] = dic[metodo] + val\n",
    "\n",
    "for metodo , val in dic.items():\n",
    "    print(f\"{metodo} , {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a248cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /salida/pro403\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /salida/pro403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f8778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-03 17:18:34,876 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper4.py, reducer4.py, /tmp/hadoop-unjar3806706490349814775/] [] /tmp/streamjob1109680042581956835.jar tmpDir=null\n",
      "2025-12-03 17:18:35,486 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 17:18:35,595 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 17:18:35,764 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764778124251_0015\n",
      "2025-12-03 17:18:36,106 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-03 17:18:36,182 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-03 17:18:36,295 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764778124251_0015\n",
      "2025-12-03 17:18:36,295 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-03 17:18:36,459 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-03 17:18:36,459 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-03 17:18:36,514 INFO impl.YarnClientImpl: Submitted application application_1764778124251_0015\n",
      "2025-12-03 17:18:36,551 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764778124251_0015/\n",
      "2025-12-03 17:18:36,554 INFO mapreduce.Job: Running job: job_1764778124251_0015\n",
      "2025-12-03 17:18:40,641 INFO mapreduce.Job: Job job_1764778124251_0015 running in uber mode : false\n",
      "2025-12-03 17:18:40,642 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-03 17:18:45,931 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-03 17:18:49,954 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-03 17:18:49,961 INFO mapreduce.Job: Job job_1764778124251_0015 completed successfully\n",
      "2025-12-03 17:18:50,021 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1199\n",
      "\t\tFILE: Number of bytes written=944754\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=28593\n",
      "\t\tHDFS: Number of bytes written=48\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4036\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1807\n",
      "\t\tTotal time spent by all map tasks (ms)=4036\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1807\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4036\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1807\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4132864\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1850368\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=993\n",
      "\t\tMap output materialized bytes=1205\n",
      "\t\tInput split bytes=176\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=1205\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=141\n",
      "\t\tCPU time spent (ms)=1720\n",
      "\t\tPhysical memory (bytes) snapshot=970838016\n",
      "\t\tVirtual memory (bytes) snapshot=7778439168\n",
      "\t\tTotal committed heap usage (bytes)=781713408\n",
      "\t\tPeak Map Physical memory (bytes)=360452096\n",
      "\t\tPeak Map Virtual memory (bytes)=2590748672\n",
      "\t\tPeak Reduce Physical memory (bytes)=252846080\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2597605376\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=28417\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=48\n",
      "2025-12-03 17:18:50,021 INFO streaming.StreamJob: Output directory: /salida/pro403\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar -file mapper4.py -file reducer4.py -mapper mapper4.py -reducer reducer4.py -input /pro403/logs.log -output /salida/pro403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5509d081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DELETE , 23\t\n",
      "\"GET , 25\t\n",
      "\"POST , 24\t\n",
      "\"PUT , 28\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /salida/pro403/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7acff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper5.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "\n",
    "    line = line.strip().split()\n",
    "\n",
    "    fecha = line[3]\n",
    "\n",
    "    hora = fecha[13:15]\n",
    "    \n",
    "   \n",
    "\n",
    "    print(f\"{hora}, 1\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ad7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer5.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "import sys \n",
    "\n",
    "dic = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "\n",
    "    hora , val = line.strip().split(\",\",1)\n",
    "    val = int(val)\n",
    "\n",
    "\n",
    "    if hora not in dic:\n",
    "        dic[hora] = val  \n",
    "    else: \n",
    "        dic[hora] = dic[hora] + val\n",
    "\n",
    "for hora , val in dic.items():\n",
    "    print(f\"{hora} , {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cf5552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /salida/pro403\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /salida/pro403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b02a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-03 17:18:54,163 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper5.py, reducer5.py, /tmp/hadoop-unjar4361864957979567555/] [] /tmp/streamjob6769994024710750708.jar tmpDir=null\n",
      "2025-12-03 17:18:54,791 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 17:18:54,914 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 17:18:55,082 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764778124251_0016\n",
      "2025-12-03 17:18:55,411 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-03 17:18:55,478 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-03 17:18:55,592 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764778124251_0016\n",
      "2025-12-03 17:18:55,592 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-03 17:18:55,782 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-03 17:18:55,782 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-03 17:18:55,848 INFO impl.YarnClientImpl: Submitted application application_1764778124251_0016\n",
      "2025-12-03 17:18:55,881 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764778124251_0016/\n",
      "2025-12-03 17:18:55,882 INFO mapreduce.Job: Running job: job_1764778124251_0016\n",
      "2025-12-03 17:18:59,969 INFO mapreduce.Job: Job job_1764778124251_0016 running in uber mode : false\n",
      "2025-12-03 17:18:59,970 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-03 17:19:05,066 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-03 17:19:09,089 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-03 17:19:09,098 INFO mapreduce.Job: Job job_1764778124251_0016 completed successfully\n",
      "2025-12-03 17:19:09,196 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=906\n",
      "\t\tFILE: Number of bytes written=944168\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=28593\n",
      "\t\tHDFS: Number of bytes written=10\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4115\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1658\n",
      "\t\tTotal time spent by all map tasks (ms)=4115\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1658\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4115\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1658\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4213760\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1697792\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=700\n",
      "\t\tMap output materialized bytes=912\n",
      "\t\tInput split bytes=176\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=912\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=151\n",
      "\t\tCPU time spent (ms)=1960\n",
      "\t\tPhysical memory (bytes) snapshot=954535936\n",
      "\t\tVirtual memory (bytes) snapshot=7781597184\n",
      "\t\tTotal committed heap usage (bytes)=773324800\n",
      "\t\tPeak Map Physical memory (bytes)=360472576\n",
      "\t\tPeak Map Virtual memory (bytes)=2592256000\n",
      "\t\tPeak Reduce Physical memory (bytes)=253636608\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2598174720\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=28417\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=10\n",
      "2025-12-03 17:19:09,196 INFO streaming.StreamJob: Output directory: /salida/pro403\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar -file mapper5.py -file reducer5.py -mapper mapper5.py -reducer reducer5.py -input /pro403/logs.log -output /salida/pro403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2ed6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 , 100\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /salida/pro403/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f0ea71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper6.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper6.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "\n",
    "    line = line.strip().split()\n",
    "\n",
    "    code = int(line[8])\n",
    "\n",
    "    if code >= 400:\n",
    "        print(f\"{line[10]} , ({0}, {1})\")\n",
    "    else:\n",
    "        print(f\"{line[10]} , ({1}, {0})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "24cecab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer6.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer6.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "import sys \n",
    "\n",
    "total = 0\n",
    "sum = 0 \n",
    "\n",
    "for line in sys.stdin:\n",
    "\n",
    "    url , code = line.strip().split(\",\",1)\n",
    "    code = code.strip()\n",
    "    if code == \"(1, 0)\":\n",
    "        sum = sum + 1\n",
    "    total = total + 1\n",
    "\n",
    "\n",
    "porcentaje = (sum / total) * 100\n",
    "\n",
    "print(f\"El porcentaje de errores es {porcentaje}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5305e6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /salida/pro403\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /salida/pro403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c1e43980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-03 17:39:41,270 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper6.py, reducer6.py, /tmp/hadoop-unjar3642688062347268694/] [] /tmp/streamjob5174099571239973979.jar tmpDir=null\n",
      "2025-12-03 17:39:41,817 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 17:39:41,942 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 17:39:42,116 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764778124251_0026\n",
      "2025-12-03 17:39:42,456 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-03 17:39:42,537 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-03 17:39:42,627 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764778124251_0026\n",
      "2025-12-03 17:39:42,627 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-03 17:39:42,778 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-03 17:39:42,779 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-03 17:39:42,841 INFO impl.YarnClientImpl: Submitted application application_1764778124251_0026\n",
      "2025-12-03 17:39:42,880 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764778124251_0026/\n",
      "2025-12-03 17:39:42,881 INFO mapreduce.Job: Running job: job_1764778124251_0026\n",
      "2025-12-03 17:39:47,953 INFO mapreduce.Job: Job job_1764778124251_0026 running in uber mode : false\n",
      "2025-12-03 17:39:47,954 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-03 17:39:52,214 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-03 17:39:56,240 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-03 17:39:56,248 INFO mapreduce.Job: Job job_1764778124251_0026 completed successfully\n",
      "2025-12-03 17:39:56,319 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4156\n",
      "\t\tFILE: Number of bytes written=950668\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=28593\n",
      "\t\tHDFS: Number of bytes written=35\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4058\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1698\n",
      "\t\tTotal time spent by all map tasks (ms)=4058\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1698\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4058\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1698\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4155392\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1738752\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=3950\n",
      "\t\tMap output materialized bytes=4162\n",
      "\t\tInput split bytes=176\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=4162\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=153\n",
      "\t\tCPU time spent (ms)=1930\n",
      "\t\tPhysical memory (bytes) snapshot=860749824\n",
      "\t\tVirtual memory (bytes) snapshot=7781117952\n",
      "\t\tTotal committed heap usage (bytes)=668991488\n",
      "\t\tPeak Map Physical memory (bytes)=359546880\n",
      "\t\tPeak Map Virtual memory (bytes)=2595454976\n",
      "\t\tPeak Reduce Physical memory (bytes)=252354560\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2594689024\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=28417\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=35\n",
      "2025-12-03 17:39:56,320 INFO streaming.StreamJob: Output directory: /salida/pro403\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar -file mapper6.py -file reducer6.py -mapper mapper6.py -reducer reducer6.py -input /pro403/logs.log -output /salida/pro403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f8d42040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El porcentaje de errores es 47.0%\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /salida/pro403/part-00000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
