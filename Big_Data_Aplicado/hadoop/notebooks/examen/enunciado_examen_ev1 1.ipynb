{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "169b1dba-2965-4f86-9d14-83592937e1e1",
   "metadata": {},
   "source": [
    "# BIG DATA APLICADO - Examen 1ª Evaluación\n",
    "\n",
    "**Instrucciones generales**\n",
    "\n",
    "1.\tTodas las sentencias deben ejecutarse desde la línea de comandos en las celdas que hay después del enunciado. No debes realizar ninguna tarea desde fuera de Jupyter.\n",
    "2.\tPuedes **añadir** todas las celdas que necesites siempre y cuando estén antes del siguiente enunciado.\n",
    "3.\tTodas las celdas **deben estar ejecutadas** y debe visualizarse el resultado de salida.\n",
    "4.\t**No es necesario documentar** las respuestas, simplemente debes hacer lo que se pide en el enunciado.\n",
    "5.\tSi un comando falla, explica la causa del error y cómo lo has solucionado.\n",
    "6.\tDebes entregar tanto el **notebook** (fichero `.ipynb`) como el mismo fichero convertido a **PDF** (es muy probable que si intentas convertirlo en el propio contenedor te falle por no tener instalado `pandoc`, si es así descargalo en formato `.md` o `html` y conviértelo en tu máquina física)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0846a60-c083-4e50-af7d-85d27a788537",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**NOMBRE**: Hugo Garmón Rey\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de87b69-d6ee-43de-83ce-5921d1bd405d",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Uso de HDFS (5.5 puntos de RA1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c77da-2112-4a53-8a49-9a3c4f97ef95",
   "metadata": {},
   "source": [
    "### Gestión básica y estructura (1.5 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "66e0fce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'examen'\n",
      "/media/notebooks/examen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "%cd examen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391df71c-d395-49fb-84ec-1af121ad76a7",
   "metadata": {},
   "source": [
    "**Preparación del entorno**\n",
    "\n",
    "- Crea un archivo local en tu máquina llamado `datos_alumno.txt`.\n",
    "- El contenido del archivo debe ser tu nombre completo y tu DNI, repetido en 10 líneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aee2f34-03dc-489c-8ab7-d1f72b936610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c995f545-472e-4f73-a0a4-33b787cb9bc2",
   "metadata": {},
   "source": [
    "**Creación de directorios en HDFS**\n",
    "\n",
    "- Crea la siguiente estructura de directorios dentro de HDFS:\n",
    "    - `/examen/{tus_iniciales}/entradas`\n",
    "    - `/examen/{tus_iniciales}/salidas`\n",
    "    - `/examen/{tus_iniciales}/logs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "617a3cdd-8a7d-49fe-aa24-380e74d1877e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/examen': File exists\n",
      "mkdir: `/examen/hgr': File exists\n",
      "mkdir: `/examen/hgr/entradas': File exists\n",
      "mkdir: `/examen/hgr/salidas': File exists\n",
      "mkdir: `/examen/hgr/logs': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /examen\n",
    "!hdfs dfs -mkdir /examen/hgr\n",
    "!hdfs dfs -mkdir /examen/hgr/entradas\n",
    "\n",
    "!hdfs dfs -mkdir /examen/hgr/salidas\n",
    "\n",
    "!hdfs dfs -mkdir /examen/hgr/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e3f78-7ca6-4e30-a026-bf51ee8cec43",
   "metadata": {},
   "source": [
    "**Ingesta de datos**\n",
    "\n",
    "- Sube el archivo local `datos_alumno.txt` al directorio HDFS `/examen/{tus_iniciales}/entradas`\n",
    "- Verifica que el archivo se ha subido correctamente listando el contenido del directorio\n",
    "- Verifica que el archivo se ha subido correctamente listando el contenido del archivo `datos_alumno.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ad6fa936-626d-4a2d-b81e-f8741c50368d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/examen/hgr/entradas/datos_alumno.txt': File exists\n",
      "Found 2 items\n",
      "-rw-r--r--   3 root supergroup    6622610 2025-12-04 09:26 /examen/hgr/entradas/clean_file_bueno.csv\n",
      "-rw-r--r--   3 root supergroup        278 2025-12-04 09:09 /examen/hgr/entradas/datos_alumno.txt\n",
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put ./datos_alumno.txt /examen/hgr/entradas\n",
    "!hdfs dfs -ls /examen/hgr/entradas\n",
    "\n",
    "!hdfs dfs -cat /examen/hgr/entradas/datos_alumno.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e37ba3-f436-4b23-879b-42afe764555a",
   "metadata": {},
   "source": [
    "### Manipulación y exploración (1.5 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bced13-a4e8-4443-8f7c-c27aa0d3cbb5",
   "metadata": {},
   "source": [
    "**Duplicación y renombrado**\n",
    "\n",
    "- Realiza una copia del archivo que acabas de subir (`datos_alumno.txt`) dentro de HDFS y colócala en la carpeta `/examen/{tus_iniciales}/salidas`.\n",
    "- Renombra esta copia en HDFS para que se llame `backup_datos.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "0604a361-d2e4-4a7c-a0f4-003d95823389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: `/examen/hgr/salidas/backup_datos.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cp /examen/hgr/entradas/datos_alumno.txt /examen/hgr/salidas/backup_datos.txt\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a438034-0de2-4f59-8edb-2e5703c99a24",
   "metadata": {},
   "source": [
    "**Inspección de contenido**\n",
    "\n",
    "- Muestra por consola las últimas 3 líneas del archivo `backup_datos.txt` que reside en HDFS\n",
    "- Muestra el tamaño total (en formato legible para los humanos) del directorio `/examen/{tus_iniciales}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "a9c98db7-62a7-4b58-926c-4e4f2c65aa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X\n",
      "Hugo Garmón Rey 71724866X"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail /examen/hgr/salidas/backup_datos.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f77d34-2620-443e-861b-4fefce665335",
   "metadata": {},
   "source": [
    "**Movimiento de datos**\n",
    "\n",
    "- Mueve el archivo original `/examen/{tus_iniciales}/entradas/datos_alumno.txt` a la carpeta `/examen/{tus_iniciales}/logs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "64a2f89d-2cf5-4fcd-be21-66279dd53db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: `/examen/hgr/logs/datos_alumno.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mv /examen/hgr/entradas/datos_alumno.txt /examen/hgr/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf2383-ecf5-4e10-a6b1-79b59db2afaf",
   "metadata": {},
   "source": [
    "### Administración avanzada (2.5 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76470ab0-8733-44b5-ba6b-5e508a16a763",
   "metadata": {},
   "source": [
    "**Factor de replicación**\n",
    "\n",
    "- Cambia el factor de replicación del archivo `/examen/{tus_iniciales}/salidas/backup_datos.txt` a **1**.\n",
    "- Comprueba que el cambio se ha efectuado correctamente utilizando el comando `fsck` o `ls` con los parámetros adecuados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "79e2df60-c142-43fb-85f3-28fca588edcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication 1 set: /examen/hgr/salidas/backup_datos.txt\n",
      "Waiting for /examen/hgr/salidas/backup_datos.txt ...\n",
      "WARNING: the waiting time may be long for DECREASING the number of replications.\n",
      ". done\n",
      "Found 3 items\n",
      "-rw-r--r--   1 root supergroup        278 2025-12-04 10:16 /examen/hgr/salidas/backup_datos.txt\n",
      "drwxr-xr-x   - root supergroup          0 2025-12-04 10:19 /examen/hgr/salidas/mr\n",
      "drwxr-xr-x   - root supergroup          0 2025-12-04 10:17 /examen/hgr/salidas/mr2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -setrep -w 1 /examen/hgr/salidas/backup_datos.txt\n",
    "\n",
    "\n",
    "!hdfs dfs -ls /examen/hgr/salidas/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51845d15-7916-48fb-aed5-dfdc34fa33ab",
   "metadata": {},
   "source": [
    "**Permisos**\n",
    "\n",
    "- Cambia los permisos del directorio `/examen/{tus_iniciales}/logs` para que solo el propietario tenga permisos de lectura, escritura y ejecución. El resto de los usuarios no debe tener acceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "19d40e0a-cb1c-4f81-9343-1c966e8ae87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxr-xr-x   - root supergroup          0 2025-12-04 09:26 /examen/hgr/entradas\n",
      "d---------   - root supergroup          0 2025-12-04 08:59 /examen/hgr/logs\n",
      "drwxr-xr-x   - root supergroup          0 2025-12-04 10:19 /examen/hgr/salidas\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -chown root /examen/hgr/logs\n",
    "!hdfs dfs -chmod a-rwx /examen/hgr/logs\n",
    "!hdfs dfs -ls /examen/hgr/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e86afb0-0e57-497e-a10d-74049dc9b7ff",
   "metadata": {},
   "source": [
    "**Gestión de cuotas**\n",
    "\n",
    "- Asigna una cuota de espacio al directorio `/examen/{tus_iniciales}/entradas` limitada a 1 MB.\n",
    "- Intenta subir un archivo (o varios) que superen en total 1 MB a ese directorio para demostrar que la cuota funciona.\n",
    "- Elimina la cuota de espacio asignada al directorio `/examen/{tus_iniciales}/entradas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "47f29762-dd65-4b3e-8b4b-f0584d23a839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /examen/hgr/entradas/clean_file_bueno.csv\n",
      "put: The DiskSpace quota of /examen/hgr/entradas is exceeded: quota = 1048576 B = 1 MB but diskspace consumed = 402654018 B = 384.00 MB\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /examen/hgr/entradas/clean_file_bueno.csv\n",
    "!hdfs dfsadmin -setSpaceQuota 1m /examen/hgr/entradas\n",
    "!hdfs dfs -put ./clean_file_bueno.csv /examen/hgr/entradas\n",
    "!hdfs dfsadmin -setSpaceQuota 1000g /examen/hgr/entradas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea69529-2528-49d7-a5f5-269182dc3cb4",
   "metadata": {},
   "source": [
    "**Snapshots y recuperación**\n",
    "\n",
    "- Habilita la funcionalidad de snapshots en el directorio `/examen/{tus_iniciales}/salidas`.\n",
    "- Crea un snapshot del directorio `/examen/{tus_iniciales}/salidas` llamado `snap_seguridad_v1`.\n",
    "- Simula un error humano borrando el archivo `/examen/{tus_iniciales}/salidas/backup_datos.txt`.\n",
    "- Recupera el archivo borrado restaurándolo desde el snapshot creado anteriormente.\n",
    "- Comprueba que el archivo vuelve a aparecer en su ubicación original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "325f3d6a-107a-45df-b244-1b237e4192f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowing snapshot on /examen/hgr/salidas succeeded\n",
      "createSnapshot: Failed to add snapshot: there is already a snapshot with the same name \"snapshot\".\n",
      "Deleted /examen/hgr/salidas/backup_datos.txt\n",
      "Found 3 items\n",
      "-rw-r--r--   3 root supergroup        278 2025-12-04 10:25 /examen/hgr/salidas/backup_datos.txt\n",
      "drwxr-xr-x   - root supergroup          0 2025-12-04 10:24 /examen/hgr/salidas/mr\n",
      "drwxr-xr-x   - root supergroup          0 2025-12-04 10:23 /examen/hgr/salidas/mr2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfsadmin -allowSnapshot /examen/hgr/salidas\n",
    "!hdfs dfs -createSnapshot /examen/hgr/salidas snapshot\n",
    "!hdfs dfs -rm -r /examen/hgr/salidas/backup_datos.txt\n",
    "!hdfs dfs -cp /examen/hgr/salidas/.snapshot/snapshot/backup_datos.txt /examen/hgr/salidas/\n",
    "!hdfs dfs -ls /examen/hgr/salidas/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5d6c4-c629-4b0e-b9c9-c7267ad7e5c0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a4eb7-106c-47ab-9390-95dc94bdafab",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Computación distribuida con MapReduce (10 puntos de RA2)\n",
    "\n",
    "Esta parte del examen la vamos a hacer con el *dataset* que puedes encontrar en [https://www.kaggle.com/datasets/ashpalsingh1525/imdb-movies-dataset](https://www.kaggle.com/datasets/ashpalsingh1525/imdb-movies-dataset) y que contiene datos sobre más de 10000 películas de IMDB. El fichero del *dataset* te lo habrá facilitado el profesor junto con el examen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d8f28-bb46-4813-885a-15ebc1e7f332",
   "metadata": {},
   "source": [
    "## Número de películas por género"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306debcd-a30c-4b71-9456-c1adec9aa9fd",
   "metadata": {},
   "source": [
    "**Número de películas de cada género**\n",
    "\n",
    "Queremos saber **cuántas películas hay en cada uno de los géneros**. Ten en cuenta que muchas películas pertenecen a más de un género. Consejo: antes de empezar observa y familiarízate con la estructura de los datos del fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "ea935c2a-42b3-4cea-9ad2-4b3aa1a512fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put ./clean_file_bueno.csv /examen/hgr/entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "46639124-a8c7-40c2-8945-e889bbffd51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "first = True\n",
    "\n",
    "for line in sys.stdin:\n",
    "\n",
    "    if first == True:\n",
    "        first = False\n",
    "        continue\n",
    "\n",
    "    line = line.strip().split(\",\")\n",
    "    print(f\"{line[3]},1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "7a66d8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "dic = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    genero , val = line.strip().split(\",\",1)\n",
    "    generos = genero.strip().split(\";\")\n",
    "    val = int(val)\n",
    "\n",
    "    for genero in generos:\n",
    "        \n",
    "        if genero in dic:\n",
    "            dic[genero] = dic[genero] + 1\n",
    "        else :\n",
    "            dic[genero] = val\n",
    "for genero , val  in dic.items():\n",
    "    print(f\"{genero}, {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "ba1cbe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", 85\n",
      "Action, 2752\n",
      "Adventure, 1890\n",
      "Animation, 1468\n",
      "Comedy, 2943\n",
      "Family, 1407\n",
      "Science Fiction, 1261\n",
      "Romance, 1576\n",
      "Crime, 1272\n",
      "Mystery, 862\n",
      "Drama, 3812\n",
      "Fantasy, 1382\n",
      "Horror, 1554\n",
      "Thriller, 2605\n",
      "War, 282\n",
      "Western, 131\n",
      "TV Movie, 212\n",
      "History, 422\n",
      "Documentary, 217\n",
      "Music, 277\n"
     ]
    }
   ],
   "source": [
    "!cat clean_file_bueno.csv | python3 mapper1.py | sort | python3 reducer1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "0dde5b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /examen/hgr/salidas/mr\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /examen/hgr/salidas/mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "5b8310a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04 10:25:32,086 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper1.py, reducer1.py, /tmp/hadoop-unjar5441073748298945732/] [] /tmp/streamjob6904871010616395613.jar tmpDir=null\n",
      "2025-12-04 10:25:32,723 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-04 10:25:32,855 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-04 10:25:33,046 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764837967362_0025\n",
      "2025-12-04 10:25:33,404 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-04 10:25:33,496 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-04 10:25:33,625 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764837967362_0025\n",
      "2025-12-04 10:25:33,625 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-04 10:25:33,819 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-04 10:25:33,820 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-04 10:25:33,905 INFO impl.YarnClientImpl: Submitted application application_1764837967362_0025\n",
      "2025-12-04 10:25:33,949 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764837967362_0025/\n",
      "2025-12-04 10:25:33,951 INFO mapreduce.Job: Running job: job_1764837967362_0025\n",
      "2025-12-04 10:25:39,039 INFO mapreduce.Job: Job job_1764837967362_0025 running in uber mode : false\n",
      "2025-12-04 10:25:39,041 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-04 10:25:43,109 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-04 10:25:48,137 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-04 10:25:48,144 INFO mapreduce.Job: Job job_1764837967362_0025 completed successfully\n",
      "2025-12-04 10:25:48,236 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=261544\n",
      "\t\tFILE: Number of bytes written=1465543\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6626932\n",
      "\t\tHDFS: Number of bytes written=288\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4769\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2147\n",
      "\t\tTotal time spent by all map tasks (ms)=4769\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2147\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4769\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2147\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4883456\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2198528\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10179\n",
      "\t\tMap output records=10177\n",
      "\t\tMap output bytes=241184\n",
      "\t\tMap output materialized bytes=261550\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2304\n",
      "\t\tReduce shuffle bytes=261550\n",
      "\t\tReduce input records=10177\n",
      "\t\tReduce output records=20\n",
      "\t\tSpilled Records=20354\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=133\n",
      "\t\tCPU time spent (ms)=3130\n",
      "\t\tPhysical memory (bytes) snapshot=983457792\n",
      "\t\tVirtual memory (bytes) snapshot=7780601856\n",
      "\t\tTotal committed heap usage (bytes)=776994816\n",
      "\t\tPeak Map Physical memory (bytes)=359743488\n",
      "\t\tPeak Map Virtual memory (bytes)=2591539200\n",
      "\t\tPeak Reduce Physical memory (bytes)=264773632\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2597961728\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6626706\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=288\n",
      "2025-12-04 10:25:48,236 INFO streaming.StreamJob: Output directory: /examen/hgr/salidas/mr\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar -file mapper1.py -file reducer1.py -mapper mapper1.py -reducer reducer1.py -input /examen/hgr/entradas/clean_file_bueno.csv -output /examen/hgr/salidas/mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "b20251c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", 85\t\n",
      "Action, 2752\t\n",
      "Adventure, 1890\t\n",
      "Animation, 1468\t\n",
      "Comedy, 2942\t\n",
      "Family, 1407\t\n",
      "Science Fiction, 1261\t\n",
      "Romance, 1575\t\n",
      "Crime, 1272\t\n",
      "Mystery, 862\t\n",
      "Drama, 3812\t\n",
      "Fantasy, 1382\t\n",
      "Horror, 1554\t\n",
      "Thriller, 2605\t\n",
      "War, 282\t\n",
      "Western, 131\t\n",
      "TV Movie, 212\t\n",
      "History, 422\t\n",
      "Documentary, 217\t\n",
      "Music, 277\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /examen/hgr/salidas/mr/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39296e75",
   "metadata": {},
   "source": [
    "**Género más popular**\n",
    "\n",
    "Utilizando MapReduce, averigua cuál es el género más popular. Debes utilizar un segundo proceso MapReduce para procesar la salida del anterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "3ff2f936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:    \n",
    "    word, val = line.strip().split(\",\",1)\n",
    "    val = int(val)\n",
    "\n",
    "    val = f\"{val:04d}\"\n",
    "\n",
    "    print(f\"{val},{word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "a2105b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "pelimax = \" \"\n",
    "valmax = 0\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "\n",
    "    val , resto = line.strip().split(\",\",1)\n",
    "    val = int(val)\n",
    "    \n",
    "    if val > valmax:\n",
    "        pelimax = resto\n",
    "        valmax = val\n",
    "\n",
    "print(f\"{valmax},{pelimax}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "e1c1e6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /examen/hgr/salidas/mr2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /examen/hgr/salidas/mr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "7aed96c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04 10:25:52,666 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper2.py, reducer2.py, /tmp/hadoop-unjar7890443765590105073/] [] /tmp/streamjob948128450267019474.jar tmpDir=null\n",
      "2025-12-04 10:25:53,340 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-04 10:25:53,512 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-04 10:25:53,743 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764837967362_0026\n",
      "2025-12-04 10:25:54,104 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-04 10:25:54,171 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-04 10:25:54,265 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764837967362_0026\n",
      "2025-12-04 10:25:54,265 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-04 10:25:54,410 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-04 10:25:54,410 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-04 10:25:54,471 INFO impl.YarnClientImpl: Submitted application application_1764837967362_0026\n",
      "2025-12-04 10:25:54,504 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764837967362_0026/\n",
      "2025-12-04 10:25:54,507 INFO mapreduce.Job: Running job: job_1764837967362_0026\n",
      "2025-12-04 10:25:59,598 INFO mapreduce.Job: Job job_1764837967362_0026 running in uber mode : false\n",
      "2025-12-04 10:25:59,600 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-04 10:26:03,646 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-04 10:26:08,692 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-04 10:26:08,700 INFO mapreduce.Job: Job job_1764837967362_0026 completed successfully\n",
      "2025-12-04 10:26:08,776 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=323\n",
      "\t\tFILE: Number of bytes written=943077\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=642\n",
      "\t\tHDFS: Number of bytes written=12\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4414\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1855\n",
      "\t\tTotal time spent by all map tasks (ms)=4414\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1855\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4414\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1855\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4519936\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1899520\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=20\n",
      "\t\tMap output records=20\n",
      "\t\tMap output bytes=277\n",
      "\t\tMap output materialized bytes=329\n",
      "\t\tInput split bytes=210\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce shuffle bytes=329\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=40\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=138\n",
      "\t\tCPU time spent (ms)=1900\n",
      "\t\tPhysical memory (bytes) snapshot=969936896\n",
      "\t\tVirtual memory (bytes) snapshot=7780216832\n",
      "\t\tTotal committed heap usage (bytes)=779091968\n",
      "\t\tPeak Map Physical memory (bytes)=357670912\n",
      "\t\tPeak Map Virtual memory (bytes)=2591125504\n",
      "\t\tPeak Reduce Physical memory (bytes)=255938560\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2599071744\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=432\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=12\n",
      "2025-12-04 10:26:08,776 INFO streaming.StreamJob: Output directory: /examen/hgr/salidas/mr2\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar  -file mapper2.py -file reducer2.py -mapper mapper2.py -reducer reducer2.py -input /examen/hgr/salidas/mr/part-00000 -output /examen/hgr/salidas/mr2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "83ef2da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3812,Drama\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /examen/hgr/salidas/mr2/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95790afb-2bbd-485f-80d4-83037bb9084b",
   "metadata": {},
   "source": [
    "**País con películas más rentables**\n",
    "\n",
    "Queremos saber qué país tiene una filmografía más rentable (ten en cuenta que *budget*=presupuesto, *revenue*=ingresos), así que tienes que obtener un listado de países y beneficios promedio por película ((total ingresos - total presupuestos) / número películas de ese país)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "7ba39163-386d-4cd7-8688-34dc078f3eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "first = True\n",
    "\n",
    "for line in sys.stdin:\n",
    "\n",
    "    if first == True:\n",
    "        first = False\n",
    "        continue\n",
    "\n",
    "    line = line.strip().split(\",\")\n",
    "    print(f\"{line[9]},{line[10]},{line[11]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "8d94111c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "pais_actual = None\n",
    "presu_total = 0\n",
    "bene_total = 0\n",
    "cont = 0\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    pais , presu , bene   = line.strip().split(\",\",2)\n",
    "    presu = float(presu)\n",
    "    bene = float(bene)\n",
    "\n",
    "    if pais_actual == None:\n",
    "        pais_actual = pais\n",
    "    \n",
    "    if pais_actual != pais:\n",
    "        rent = (bene_total - presu_total) / cont\n",
    "        print(f\"{pais_actual} , {round(rent,2)}\")\n",
    "        presu_total = 0\n",
    "        bene_total = 0\n",
    "        cont = 0\n",
    "        pais_actual = pais\n",
    "    \n",
    "    if pais_actual == pais:\n",
    "        presu_total = presu_total + presu\n",
    "        bene_total = bene_total + bene\n",
    "        cont = cont + 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "7da63dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danish , -12404673.0\n",
      "English , 172569423.67\n",
      "French , 290021646.03\n",
      "German , -35138261.87\n",
      "Italian , 818268745.8\n",
      "Japanese , 649803469.8\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/notebooks/examen/reducer3.py\", line 12, in <module>\n",
      "    presu = float(presu)\n",
      "ValueError: could not convert string to float: ' Released'\n",
      "sort: write failed: 'standard output': Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!cat clean_file_bueno.csv | python3 mapper3.py | sort | python3 reducer3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "cb72790a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /examen/hgr/salidas/mr\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /examen/hgr/salidas/mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "9a664054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04 10:26:13,558 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper3.py, reducer3.py, /tmp/hadoop-unjar1594897202218193005/] [] /tmp/streamjob9214315745304873431.jar tmpDir=null\n",
      "2025-12-04 10:26:14,334 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-04 10:26:14,511 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-04 10:26:14,776 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764837967362_0027\n",
      "2025-12-04 10:26:15,131 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-04 10:26:15,190 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-04 10:26:15,303 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764837967362_0027\n",
      "2025-12-04 10:26:15,303 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-04 10:26:15,446 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-04 10:26:15,446 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-04 10:26:15,505 INFO impl.YarnClientImpl: Submitted application application_1764837967362_0027\n",
      "2025-12-04 10:26:15,537 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764837967362_0027/\n",
      "2025-12-04 10:26:15,538 INFO mapreduce.Job: Running job: job_1764837967362_0027\n",
      "2025-12-04 10:26:20,629 INFO mapreduce.Job: Job job_1764837967362_0027 running in uber mode : false\n",
      "2025-12-04 10:26:20,629 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-04 10:26:25,720 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-04 10:26:28,742 INFO mapreduce.Job: Task Id : attempt_1764837967362_0027_r_000000_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeReducer.reduce(PipeReducer.java:127)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:445)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:393)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "\n",
      "2025-12-04 10:26:33,777 INFO mapreduce.Job: Task Id : attempt_1764837967362_0027_r_000000_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeReducer.reduce(PipeReducer.java:127)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:445)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:393)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "\n",
      "2025-12-04 10:26:38,810 INFO mapreduce.Job: Task Id : attempt_1764837967362_0027_r_000000_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeReducer.reduce(PipeReducer.java:127)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:445)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:393)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "\n",
      "2025-12-04 10:26:44,842 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-04 10:26:45,855 INFO mapreduce.Job: Job job_1764837967362_0027 failed with state FAILED due to: Task failed task_1764837967362_0027_r_000000\n",
      "Job failed as tasks failed. failedMaps:0 failedReduces:1 killedMaps:0 killedReduces: 0\n",
      "\n",
      "2025-12-04 10:26:45,920 INFO mapreduce.Job: Counters: 40\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=918014\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6626932\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=6\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tFailed reduce tasks=4\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5338\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8577\n",
      "\t\tTotal time spent by all map tasks (ms)=5338\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8577\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5338\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8577\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5466112\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=8782848\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10179\n",
      "\t\tMap output records=10177\n",
      "\t\tMap output bytes=269312\n",
      "\t\tMap output materialized bytes=289678\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=10177\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=122\n",
      "\t\tCPU time spent (ms)=1950\n",
      "\t\tPhysical memory (bytes) snapshot=721874944\n",
      "\t\tVirtual memory (bytes) snapshot=5181038592\n",
      "\t\tTotal committed heap usage (bytes)=583532544\n",
      "\t\tPeak Map Physical memory (bytes)=361385984\n",
      "\t\tPeak Map Virtual memory (bytes)=2591588352\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6626706\n",
      "2025-12-04 10:26:45,920 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar -file mapper3.py -file reducer3.py -mapper mapper3.py -reducer reducer3.py -input /examen/hgr/entradas/clean_file_bueno.csv -output /examen/hgr/salidas/mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "e3b5ca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: `/examen/hgr/salidas/mr/part-00000': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /examen/hgr/salidas/mr/part-00000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
