{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7950a9f9-ebef-48f1-b7ef-74689ce05ae5",
   "metadata": {},
   "source": [
    "# SISTEMAS DE BIG DATA - Examen 1ª Evaluación\n",
    "\n",
    "**Instrucciones generales**\n",
    "\n",
    "1.\tTodas las sentencias deben ejecutarse desde la línea de comandos en las celdas que hay después del enunciado. No debes realizar ninguna tarea desde fuera de Jupyter.\n",
    "2.\tPuedes **añadir** todas las celdas que necesites siempre y cuando estén antes del siguiente enunciado.\n",
    "3.\tTodas las celdas **deben estar ejecutadas** y debe visualizarse el resultado de salida.\n",
    "4.\t**No es necesario documentar** las respuestas, simplemente debes hacer lo que se pide en el enunciado.\n",
    "5.\tDespués de cada parte debes insertar una **captura de pantalla** del cliente gráfico de la base de datos correspondientes donde se vea que los datos se han cargado correctamente.\n",
    "6.\tDebes entregar tanto el **notebook** (fichero `.ipynb`) como el mismo fichero convertido a **PDF** (es muy probable que si intentas convertirlo en el propio contenedor te falle por no tener instalado `pandoc`, si es así descargalo en formato `.md` o `html` y conviértelo en tu máquina física)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ee366-1158-4061-a6f1-ec4c6fde2148",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**NOMBRE**: Hugo Garmón Rey\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d393dc33-a532-4bba-b10a-28d4f9e76d2e",
   "metadata": {},
   "source": [
    "## Contexto del escenario\n",
    "\n",
    "Has sido contratado por una fábrica inteligente que dispone de sensores de temperatura y vibración en sus máquinas críticas. La empresa necesita un sistema backend capaz de procesar los datos que llegan de los sensores en tiempo real.\n",
    "\n",
    "El sistema debe cumplir dos objetivos simultáneos:\n",
    "\n",
    "1.  **Monitorización en vivo (Dashboard):** los operarios necesitan saber el estado *actual* de cada máquina y si hay alguna alarma activa en este preciso instante. Para esto usarás **Redis**.\n",
    "2.  **Histórico para mantenimiento predictivo:** el equipo de Data Science necesita almacenar todos los datos brutos a lo largo del tiempo para entrenar modelos de IA futuros. Para esto usarás **InfluxDB**.\n",
    "\n",
    "## Los Datos de Entrada\n",
    "\n",
    "Los datos con los que vas a trabajar los tienes en el *dataset* sintético adjunto llamado `sensores.csv`. Este *dataset* contiene lecturas simuladas con las siguientes columnas:\n",
    "\n",
    "  - `timestamp`: fecha y hora del evento.\n",
    "  - `machine_id`: identificador único de la máquina.\n",
    "  - `zone`: zona de la fábrica.\n",
    "  - `temperature`: temperatura en grados Celsius.\n",
    "  - `vibration`: nivel de vibración (0-100).\n",
    "  - `lat`, `lon`: coordenadas del robot.\n",
    "  - `status`: estado reportado por la máquina (\"OK\", \"WARNING\", \"ERROR\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e1bde7-5156-4b81-a5c3-4e903f190a0c",
   "metadata": {},
   "source": [
    "**IMPORTANTE**\n",
    "\n",
    "El desarrollo del examen debe de ser modular, con un programa principal que inicialice las conexiones a la base de datos y lea los datos del fichero y luego invocará **una función diferente para cargar cada tipo de dato** en la base de datos\n",
    "\n",
    "Es decision tuya elegir los parámetros que recibirá cada función, aunque es altamente aconsejable **no utilizar variables globales**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c1c3d1-5976-44e0-8f12-b0e36ed809cd",
   "metadata": {},
   "source": [
    "## Parte A: Persistencia histórica (InfluxDB)\n",
    "\n",
    "`2 puntos`\n",
    "\n",
    "En esta parte tienes que crear un script que lea el fichero CSV facilitado y almacene los datos en una base de datos InfluxDB.\n",
    "\n",
    "Los aspectos que tienes que tener en cuenta son:\n",
    "\n",
    "  - **Bucket:** `factory_logs`\n",
    "  - **Measurement:** `maquinaria`\n",
    "  - **Requisito clave:** debes modelar correctamente los datos usando adecuadamente *tags* o *fields* según el tipo de datos. Se debe respetar el `timestamp` del datos (no usar el tiempo de ingesta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "afc05b75-32a2-444d-9ff5-5463c45a1142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que carga los datos en InfluxDB\n",
    "\n",
    "import influxdb_client\n",
    "from datetime import datetime,timezone\n",
    "from influxdb_client import Point, WriteOptions\n",
    "\n",
    "writeoptions = WriteOptions(\n",
    "        batch_size = 500,\n",
    "        flush_interval = 1,\n",
    "        write_type = \"SYNCHRONOUS\"\n",
    "    )\n",
    "\n",
    "\n",
    "writre_api = client.write_api(write_options=writeoptions)\n",
    "\n",
    "def cargarDatos(csv_):\n",
    "    with open(csv_) as f:\n",
    "        lector = csv.reader(f)\n",
    "\n",
    "        next(lector)\n",
    "\n",
    "        for row in lector:\n",
    "            date_ = row[0]\n",
    "            dt = datetime.strptime(date_,\"%Y-%m-%d %H:%M:%S\")\n",
    "            dt_utc = dt.replace(tzinfo=timezone.utc)\n",
    "            time_date = dt_utc.isoformat().replace('+00:00','Z')\n",
    "            machine_id = row[1]\n",
    "            zone = row[2]\n",
    "            temperature = float(row[3])\n",
    "            vibration = float(row[4])\n",
    "            lat = float(row[5])\n",
    "            lon = float(row[6])\n",
    "            status = row[7]\n",
    "    \n",
    "        \n",
    "            p = Point(\"maquinaria\").time(time_date).tag(\"machine_id\",machine_id).tag(\"zone\",zone).tag(\"status\",status) \\\n",
    "            .field(\"lat\",lat).field(\"lon\",lon).field(\"temperature\",temperature).field(\"vibration\",vibration) \\\n",
    "        \n",
    "        \n",
    "            writre_api.write(bucket=\"factory_logs\" , org=\"docs\" , record=p)\n",
    "\n",
    "\n",
    "writre_api.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "75ebcc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando estado de salud de InfluxDB en http://influxdb2:8086...\n",
      "Conexión exitosa\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import csv\n",
    "\n",
    "\n",
    "from influxdb_client.client.write_api import ASYNCHRONOUS\n",
    "from influxdb_client.client.exceptions import InfluxDBError\n",
    "from urllib3.exceptions import NewConnectionError\n",
    "\n",
    "\n",
    "data = os.listdir(\"./pro206/data/crypto_files\")\n",
    "\n",
    "INFLUX_URL = \"http://influxdb2:8086\"\n",
    "INFLUX_TOKEN = \"MyInitialAdminToken0=hola\"\n",
    "\n",
    "client = None\n",
    "try:\n",
    "    client = influxdb_client.InfluxDBClient(\n",
    "        url=INFLUX_URL,\n",
    "        token=INFLUX_TOKEN,\n",
    "        org=\"docs\"\n",
    "    )\n",
    "\n",
    "    print(f\"Verificando estado de salud de InfluxDB en {INFLUX_URL}...\")\n",
    "    health = client.health()\n",
    "\n",
    "    if health.status == \"pass\":\n",
    "        print(\"Conexión exitosa\")\n",
    "    else:\n",
    "        print(\"Conexión fallida\")\n",
    "\n",
    "except(InfluxDBError,NewConnectionError) as e:\n",
    "    print(\"Error\")\n",
    "    print(e)\n",
    "\n",
    "\n",
    "cargarDatos(\"./examen/telemetria_agv.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed5349-3dab-4524-8061-ef99218adcbc",
   "metadata": {},
   "source": [
    "## Parte B - Analítica en tiempo real con Redis\n",
    "\n",
    "Debes crear un script que alimente las siguientes estructuras en Redis por cada dato procesado:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc31d35-9b8d-4c0e-9a3f-8f89f437ddcd",
   "metadata": {},
   "source": [
    "### 1.- Estadísticas agregadas\n",
    "\n",
    "`1 punto`\n",
    "\n",
    "Al procesar masivamente datos de telemetría, es costoso consultar la base de datos histórica (InfluxDB) para preguntas simples como \"¿Cuál ha sido la temperatura máxima hoy en el Almacén A?\". Vamos a usar Redis Hashes para mantener un marcador actualizado de estadísticas por zona.\n",
    "\n",
    "Para cada fila procesada del CSV, debes actualizar un Hash correspondiente a la Zona (zone) donde se encuentra el robot.\n",
    "\n",
    "- **Clave:** `stats:zone:{nombre_zona}` (Ej: stats:zone:Almacen_A, stats:zone:Recepcion...).\n",
    "- **Campos:**:\n",
    "    - `total_lecturas`: contador total de datos recibidos de esa zona.\n",
    "    - `total_errores`: contador de cuántas veces el status ha sido \"ERROR\".\n",
    "    - `max_temp`: La temperatura más alta registrada hasta el momento en esa zona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "422b5288-dcf7-4cd9-a886-d44f639ac796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que genera las estadísticas agregadas\n",
    "\n",
    "import os \n",
    "import csv\n",
    "\n",
    "def estadisticasZona(csv_):\n",
    "   \n",
    "\n",
    "    cont = 0\n",
    "    cont_error = 0\n",
    "    max_temp = 0.0\n",
    "    zona_actual = None\n",
    "\n",
    "    with open(csv_) as f:\n",
    "        lector = csv.reader(f)\n",
    "\n",
    "        next(lector)\n",
    "\n",
    "        for row in lector:\n",
    "            zone = row[2]\n",
    "            temperature = float(row[3])\n",
    "            status = row[7]\n",
    "\n",
    "            if zona_actual == None:\n",
    "                zona_actual = zone\n",
    "                zone_key = f\"stats:zone:{zona_actual}\"\n",
    "\n",
    "            if zone == zona_actual:\n",
    "                zone_key = f\"stats:zone:{zona_actual}\"\n",
    "                cont = cont+1\n",
    "                if temperature >= max_temp:\n",
    "                    max_temp = temperature\n",
    "                if status == \"ERROR\":\n",
    "                    cont_error = cont_error+1\n",
    "\n",
    "                dic = {\n",
    "                    \"Total_lecturas\" : cont,\n",
    "                    \"Total_errores\" : cont_error,\n",
    "                    \"Max_temp\" : max_temp\n",
    "                }   \n",
    "\n",
    "            if zone != zona_actual:\n",
    "                r.hset(zone_key,mapping=dic)\n",
    "                \n",
    "                \n",
    "                zona_actual = zone \n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f03f286-7c4a-4e9e-bb24-0d10e4a3107e",
   "metadata": {},
   "source": [
    "### 2.- Ranking de \"puntos calientes\" (Sorted Set)\n",
    "\n",
    "`1 punto`\n",
    "\n",
    "El jefe de planta quiere ver en una pantalla un \"Top de Máquinas con mayor temperatura\" ordenado de mayor a menor en tiempo real.\n",
    "\n",
    "- **Estructura:** `Sorted Set` (ZSET)\n",
    "- **Clave:** `dashboard:hottest_machines`\n",
    "- **Score:** La temperatura actual (`temperature`).\n",
    "- **Member:** El ID de la máquina (`machine_id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "2ee87d30-521d-4635-8c97-32abf50680c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que carga el sorted set\n",
    "\n",
    "def cargarSorted():\n",
    "    zonas = [\"Almacen_A\",\"Almacen_B\",\"Recepción\",\"Expediciones\",\"Ensamblaje\"]\n",
    "\n",
    "    for i in zonas:\n",
    "        zone_key = f\"stats:zone:{i}\"\n",
    "        zone_data = r.hgetall(zone_key)\n",
    "        \n",
    "        for k , v in zone_data.items():\n",
    "            leaderboard_key = \"dashboard:hottest_machines\"\n",
    "            r.zadd(leaderboard_key,{zone_key : float(v)})\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ddfd7b-b7a5-4c34-b9ef-40cae6419d07",
   "metadata": {},
   "source": [
    "### 3.- Seguimiento de flota (Geospatial)\n",
    "\n",
    "`1 punto`\n",
    "\n",
    "Las máquinas de este escenario son AGVs (robots móviles) que se mueven por la planta. Necesitamos saber su ubicación exacta.\n",
    "\n",
    "- **Estructura:** `Geo`\n",
    "- **Clave:** `factory:map`\n",
    "- **Datos:** Usa la latitud y longitud que vienen en el CSV para posicionar el `machine_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ecc26b-134c-4f0e-b72e-742c2059128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que carga los datos geoespaciales\n",
    "\n",
    "def cargarDatosGeo(csv_):\n",
    "    with open(csv_) as f:\n",
    "        lector = csv.reader(f)\n",
    "\n",
    "        next(lector)\n",
    "\n",
    "        for row in lector:\n",
    "            machine_id = row[1]\n",
    "            lat = float(row[5])\n",
    "            lon = float(row[6])\n",
    "            \n",
    "            r.geoadd(\"factory:map\",(lon,lat,machine_id))\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999afa47-8719-4cd0-9c26-921de0349f5b",
   "metadata": {},
   "source": [
    "### 4.- Contadores globales atómicos (String)\n",
    "\n",
    "`1 punto`\n",
    "\n",
    "Necesitamos estadísticas rápidas que no requieran contar filas en una base de datos histórica.\n",
    "\n",
    "- **Estructura:** `String` (Contador)\n",
    "- **Clave:** `stats:total_processed` -\\> Incrementar en 1 por cada fila procesada.\n",
    "- **Clave:** `stats:total_errors` -\\> Incrementar en 1 solo si el `status` es \"ERROR\".\n",
    "- **Clave:** `stats:total_warnings` -\\> Incrementar en 1 solo si el `status` es \"WARNING\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "c47fc3d1-cd1f-4e77-8226-0e0bd571fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contadores(csv_):\n",
    "    \n",
    "    cont = 0\n",
    "    error = 0\n",
    "    war = 0\n",
    "    \n",
    "    with open(csv_) as f:\n",
    "        lector = csv.reader(f)\n",
    "\n",
    "        next(lector)\n",
    "\n",
    "        for row in lector:\n",
    "            status = row[7]            \n",
    "            if status == \"ERROR\":\n",
    "                error = error + 1\n",
    "            if status == \"WARNING\":\n",
    "                war = war + 1 \n",
    "\n",
    "            cont = cont + 1\n",
    "    \n",
    "    r.set(\"stats:total_processed\", cont)\n",
    "    r.set(\"stats:total_errors\", error)\n",
    "    r.set(\"stats:total_warning\", war)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193de2fc-b8ed-4bfc-9bb6-a7c8e6aec991",
   "metadata": {},
   "source": [
    "### 5.- Cola de anomalías críticas (List)\n",
    "\n",
    "`1 punto`\n",
    "\n",
    "Queremos tener también una cola de anomalías críticas. Por cada registro cuyo `status` sea `ERROR` deberás crear un JSON y almacenarlo en una estructura tipo FIFO:\n",
    "\n",
    "- **Estructura:** `List`\n",
    "- **Clave:** `alerts:queue`\n",
    "- **Datos:**: el JSON debe incluir: `machine_id`, `timestamp` y un mensaje: *\"Critical failure at [Lat, Lon]\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "81917494-a1c2-4dba-b4fe-64d6595b3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que carga los datos en la cola\n",
    "import json\n",
    "\n",
    "def anomalias(csv_):\n",
    "    \n",
    "    cont = 0\n",
    "    error = 0\n",
    "    war = 0\n",
    "    \n",
    "    with open(csv_) as f:\n",
    "        lector = csv.reader(f)\n",
    "\n",
    "        next(lector)\n",
    "\n",
    "        for row in lector:\n",
    "            date_ = row[0]\n",
    "            dt = datetime.strptime(date_,\"%Y-%m-%d %H:%M:%S\")\n",
    "            dt_utc = dt.replace(tzinfo=timezone.utc)\n",
    "            time_date = dt_utc.isoformat().replace('+00:00','Z')   \n",
    "            machine_id = row[1]\n",
    "            status = row[7]    \n",
    "            lat = float(row[5])\n",
    "            lon = float(row[6])  \n",
    "\n",
    "            if status == \"ERROR\":\n",
    "                dic = {\n",
    "                    \"machine_id\" : machine_id,\n",
    "                    \"timestamp\" : time_date,\n",
    "                    \"msg\" : f\"Critical failuere at {lat} {lon}\"\n",
    "                }  \n",
    "\n",
    "                dic_json = json.dumps(dic)    \n",
    "\n",
    "                r.rpush(\"alerts:queue\",dic_json)\n",
    "            \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1df4799-0663-4c9e-bb15-590e9c97441b",
   "metadata": {},
   "source": [
    "## Programa principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "ac019ce5-4388-4667-89c6-99f5f75bd8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí debes insertar el programa principal que llama al resto de funciones\n",
    "import redis \n",
    "\n",
    "r = redis.Redis(\n",
    "    host='redis',\n",
    "    port=6379,\n",
    "    db=0,\n",
    "    decode_responses=True)\n",
    "r.ping()\n",
    "\n",
    "estadisticasZona(\"./examen/telemetria_agv.csv\")\n",
    "cargarSorted()\n",
    "cargarDatosGeo(\"./examen/telemetria_agv.csv\")\n",
    "contadores(\"./examen/telemetria_agv.csv\")\n",
    "anomalias(\"./examen/telemetria_agv.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514dfa1-6f76-4b14-86e3-dce3ae6ded91",
   "metadata": {},
   "source": [
    "## Capturas de pantalla\n",
    "\n",
    "A partir de aquí tienes que insertar las capturas de pantalla correspondientes a cada punto. Las capturas de pantalla corresponderán a la interfaz gráfica de la base de datos correspondiente y se debe mostrar que los datos se han cargado correctamente. Los apartados que no tengan la captura de pantalla correspondiente **se considerarán no realizados**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45932542-a53f-4672-9680-3a735fd9d580",
   "metadata": {},
   "source": [
    "### Captura de InfluxDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b103bbdb",
   "metadata": {},
   "source": [
    "![alt text](./examen/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea0c97a-6181-4257-972f-52d2915a1e77",
   "metadata": {},
   "source": [
    "### Captura de estadísticas agregadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f4350",
   "metadata": {},
   "source": [
    "![alt text](2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab967f49-1886-4bee-953b-61a20f4bcb5b",
   "metadata": {},
   "source": [
    "### Captura de ranking de puntos calientes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff111d2",
   "metadata": {},
   "source": [
    "![alt text](3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a06cfcb-7677-425d-bc63-7e85042d8c62",
   "metadata": {},
   "source": [
    "### Captura de seguimiento de flota"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46eb268",
   "metadata": {},
   "source": [
    "![alt text](4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589bae22-0d26-4f38-9bcb-db3eec184f8c",
   "metadata": {},
   "source": [
    "### Captura de contadores globales atómicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f001c",
   "metadata": {},
   "source": [
    "![alt text](5.png)\n",
    "![alt text](6.png)\n",
    "![alt text](7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a301380-6041-411e-9289-f15b97ceac37",
   "metadata": {},
   "source": [
    "### Captura de cola de anomalías críticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50532ea4",
   "metadata": {},
   "source": [
    "![alt text](8.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
